{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExtraCredit.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "icmMGu4fxEFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A1"
      ]
    },
    {
      "metadata": {
        "id": "WAZ0w_yMxIGl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (可以)EC1: 自己写平方差loss function\n",
        "参照https://github.com/random-forests/applied-dl/blob/master/examples/2.3-custom-loss.ipynb\n",
        "\n",
        "Provide your own implementation of a squared error loss function, and run an experiment to compare it to cross entropy. What do you find?\n",
        "Note: this part was updated on Feb 3. Originally, we were going to write our own implementations of softmax and cross entropy loss, but as the docs for TF 2.0 are still underway, decided to upload a complete example of that, for you to learn from. If you did the previous version of this, no worries, you’ll get credit for it. If not, try this one.\n",
        "We talked a little bit in class about cross entropy loss, and why it’s preferable to classification error. But, we didn’t compare it squared error. Could you use squared error to train a DNN for classification? Why or why not? Using the custom loss function ​example​ as starter code, write\n",
        "     \n",
        "your own loss function for squared error (do not use built-in methods). Next, design and run an experiment to compare it cross entropy. What do you find?"
      ]
    },
    {
      "metadata": {
        "id": "ZtBt968PxsqE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC2: 自己写dense layer (subclass)\n",
        "参照https://github.com/random-forests/applied-dl/blob/master/examples/2.2-hello-subclassing.ipynb\n",
        "\n",
        "Provide your own implementation of a Dense layer.\n",
        "So far we’ve used high-level building blocks to define our layers and loss. If you look at example\n",
        "2.2​ on GitHub, you’ll find this block of code:\n",
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(10)\n",
        "The built-in Dense layers are cool, but can you write one of your own? Please do so, and use it to create your model (as opposed to the existing Dense layer class). Run a short experiment comparing your implementation to the built-in one, and include saved results in your submission.\n",
        "Tips: ​Use the code from part two as a starting point. See this ​example​ to learn how to write a custom layer."
      ]
    },
    {
      "metadata": {
        "id": "0mzIj83Wx9gl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC3: ​Visualize the learned weights (subclass)\n",
        "\n",
        "Using the Subclassing API as in part two, write and train a linear model to classify the MNIST dataset. After it’s trained, extract the learned weights from the Dense layer, and produce a visualization similar to the one below using Matplotlib.\n",
        "Hint: recall MNIST images are 28x28 = 784 unrolled pixels. These are fed into your Dense layer, which connects to 10 outputs (so you have 784 x 10 = 7840 weights). The import thing is to realize you have 784 weights per output digit (one for each pixel). What would you see if you reshaped those weights back into a 28x28 image?\n",
        "Write code to do so, and produce a visualization similar to the one below. Include it with your submission."
      ]
    },
    {
      "metadata": {
        "id": "KAOs7TxJyG-C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A2"
      ]
    },
    {
      "metadata": {
        "id": "qZ0GzF3qyLqs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## （可以）EC1: Classify QuickDraw images.\n",
        "参照：https://github.com/random-forests/applied-dl/blob/master/examples/quick-draw-loader.ipynb\n",
        "\n",
        "If you’re up for more image classification, use the ​QuickDraw Loader​ to create a dataset of a bunch of different classes (you can use “animals” to start). Write a CNN to classify these images, and report your accuracy. How accurate of a model can you train to recognize 50 classes? 100? More?"
      ]
    },
    {
      "metadata": {
        "id": "SZyk3mDsydCj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A3"
      ]
    },
    {
      "metadata": {
        "id": "9Qkat8_5yfMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC1​: Demonstrate the vanishing gradient problem. \n",
        "Write a DNN (say, 10 layers deep) and train it on a simple dataset like MNIST. Choose activation functions, initialization strategies, and an optimizer that are likely to cause this behavior. Produce histograms of the activations and gradients at various during training. What do you see?\n",
        "* Visualize gradients and activations in Matplotlib or TensorBoard overtime (note: doing so in TensorBoard may require digging deeper into the docs, or borrowing from 1.x code).\n",
        "* Run a second experiment where you take action to correct this behavior. Visualize and compare the results.\n"
      ]
    },
    {
      "metadata": {
        "id": "umDZ2uHzyrP0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## （不干）EC2​: 看paper实现activation function\n",
        "Provide your own implementation of two activation functions published within the last 18 months, and run experiments to compare the results against a built in method like ReLU. What do you see? Did your results match expectations from the papers?"
      ]
    },
    {
      "metadata": {
        "id": "6kyZnRm6y3UF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A4"
      ]
    },
    {
      "metadata": {
        "id": "G3SBRf_-y4nu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## (要用js，还是算了？) EC1​: ​Deploy your ColorBot model in the browser. Show the predicted colors visually."
      ]
    },
    {
      "metadata": {
        "id": "PEd-t7rfzN4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC2​: Generate QuickDraw drawings with an RNN. \n",
        "参照https://github.com/random-forests/applied-dl/blob/master/examples/quick-draw-loader.ipynb\n",
        "\n",
        "Bonus points: include an animated GIF showing the drawings being created step by step."
      ]
    },
    {
      "metadata": {
        "id": "DpH8eDKYzWRY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A5 可以做，但是担心训练时间长？"
      ]
    },
    {
      "metadata": {
        "id": "_oB5iPoGzX2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC1​: Investigate interlingual representations.\n",
        "Train an encoder-decoder model to translate between two source languages (say, English and Spanish) into one target language (say, French). Once the encoder is trained, use it to encode similar sentences in the source languages (e.g., “Rachel likes coffee” and “Rachel le gusta el cafe”). Investigate how the encoder compresses and represents concepts from the source sentences.\n",
        "  \n",
        "Can you find any elements in the encoded vector that correspond to specific concepts? If you use a small corpus with a tiny number of concepts, and a tiny encoding vector, it may be possible to find something cool.\n"
      ]
    },
    {
      "metadata": {
        "id": "0KNT4bVszhvb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## EC2​: Attention mechanisms.\n",
        "Implement and compare two different attention mechanisms. How do they affect your BLEU score in part three?"
      ]
    },
    {
      "metadata": {
        "id": "eMSCt9c7zNBh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qR_6YxLMyv9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52WsR1wZwHtc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}