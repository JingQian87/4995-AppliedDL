{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EC2-A5-jq2282.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faP_TfySH12q",
        "colab_type": "text"
      },
      "source": [
        "# EC2-A5: Attention mechanisms\n",
        "### Jing Qian (jq2282)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhk7LSRmH3H7",
        "colab_type": "text"
      },
      "source": [
        "## Step 1. Install packages and load libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkZU21YuHt1Y",
        "colab_type": "code",
        "outputId": "3c450326-4e24-4961-b34b-5bd492b52207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIPY-Wq6H0wK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXGdnexoH_5q",
        "colab_type": "text"
      },
      "source": [
        "## Step 2. Load and preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8x53rP9uzqC",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2diHj7jsIFOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ntrain = 1000\n",
        "ntest = 100\n",
        "nall = ntrain + ntest\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7LoioJgIO97",
        "colab_type": "code",
        "outputId": "4a9c7db2-78e1-4810-892f-e587cb96df96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLRG0JchISlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load(fname):\n",
        "  # Load the file using std open\n",
        "  f = open(fname, 'r')\n",
        "  text = []\n",
        "  for line in f.readlines():\n",
        "    text.append(line.replace('\\n','').split('\\t'))\n",
        "    \n",
        "  f.close()\n",
        "  return text\n",
        "\n",
        "data = load('/content/gdrive/My Drive/spa-eng/spa.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBcVdFi2IUaO",
        "colab_type": "code",
        "outputId": "a2d656a0-2a0e-441b-ce09-3c08b92a4384",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "print(data[10:15])\n",
        "print(np.shape(data))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Fire!', '¡Disparad!'], ['Help!', '¡Ayuda!'], ['Help!', '¡Socorro! ¡Auxilio!'], ['Help!', '¡Auxilio!'], ['Jump!', '¡Salta!']]\n",
            "(118964, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGNOKNclIYC-",
        "colab_type": "code",
        "outputId": "fdb63a90-ee6c-4a36-e207-0002cdef3399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.random.seed(10)\n",
        "shuffled_data = np.random.permutation(data)\n",
        "selected_id = np.random.randint(len(data), size = nall)\n",
        "train_data = shuffled_data[selected_id[:ntrain], :]\n",
        "print(np.shape(train_data), np.shape(shuffled_data))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 2) (118964, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skj36OdHJR-7",
        "colab_type": "text"
      },
      "source": [
        "### 2.2. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ndtl8GZZJUbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(s):\n",
        "  # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "  s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "  s = s.strip()\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeZOk_9qJktc",
        "colab_type": "code",
        "outputId": "d52bc647-6e72-43ab-af8b-66ff3785ac0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_data = [(preprocess(eng), preprocess(spa)) for (eng, spa) in train_data]\n",
        "print(train_data[0])\n",
        "train_eng, train_spa = list(zip(*train_data))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('<start> How do you think that makes me feel ? <end>', '<start> ¿ Como crees que me hace sentir eso ? <end>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yTgCvxAKKEg",
        "colab_type": "code",
        "outputId": "8b408df3-9641-43ef-f6f7-8da2fabf2281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "eng_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "eng_tokenizer.fit_on_texts(train_eng)\n",
        "train_eng = eng_tokenizer.texts_to_sequences(train_eng)\n",
        "train_eng = tf.keras.preprocessing.sequence.pad_sequences(train_eng, padding='post')\n",
        "print(train_eng[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  47  20   7  88  16 369  17 103  10   2   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5JpJftKK8gY",
        "colab_type": "code",
        "outputId": "ab0ed461-4bc4-4699-a0c5-5192d6274c42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "spa_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "spa_tokenizer.fit_on_texts(train_spa)\n",
        "train_spa = spa_tokenizer.texts_to_sequences(train_spa)\n",
        "train_spa = tf.keras.preprocessing.sequence.pad_sequences(train_spa, padding='post')\n",
        "print(train_spa[0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  10  35 127   4  15  47 193  36  11   2   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs323ojHMrM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "spa_vocab_size = len(spa_tokenizer.word_index) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0jztFXOM6S1",
        "colab_type": "code",
        "outputId": "965fb58b-2338-4ab7-c1f2-398d30584ced",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Demonstrate the correspondence between word and code, not used in model\n",
        "def decode(encoded, tokenizer):\n",
        "  for number in encoded:\n",
        "    if number !=0:\n",
        "      print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "      \n",
        "decode(train_eng[0], eng_tokenizer)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 -> <start>\n",
            "47 -> how\n",
            "20 -> do\n",
            "7 -> you\n",
            "88 -> think\n",
            "16 -> that\n",
            "369 -> makes\n",
            "17 -> me\n",
            "103 -> feel\n",
            "10 -> ?\n",
            "2 -> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0eLXu-NIoNK",
        "colab_type": "code",
        "outputId": "b65456df-015a-454f-e6ea-1f7966c17821",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Create labels for the decoder by shifting the target sequence\n",
        "# one to the right.\n",
        "target_labels = np.zeros(train_spa.shape)\n",
        "target_labels[:,0:train_spa.shape[1] -1] = train_spa[:,1:]\n",
        "\n",
        "print(\"Target sequence\", train_spa[0])\n",
        "print(\"Target label\", target_labels[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target sequence [  1  10  35 127   4  15  47 193  36  11   2   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Target label [ 10.  35. 127.   4.  15.  47. 193.  36.  11.   2.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qafjzAUJN-fN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_eng, train_spa, target_labels)).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRBV9vaOJ_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test code!\n",
        "# example_batch = next(iter(dataset))\n",
        "# source, target, taget_labels = example_batch\n",
        "# print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HCYgzctvCB0",
        "colab_type": "text"
      },
      "source": [
        "# Step 3. Model with Bahdanau Attention Mechanism "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJeIu2MZvUbT",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrHuydEsOmPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.rnn_units = rnn_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def init_state(self):\n",
        "    return tf.zeros((self.batch_size, self.rnn_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwZ96A8YrIA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TECibZsPQokh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "    \n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    \n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ6ZJc7HX1g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 32\n",
        "RNN_SIZE = 64\n",
        "BATCH_SIZE = 5\n",
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKc1fCz1Zevj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maximum sequence length\n",
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "  \n",
        "max_length_eng, max_length_spa = max_length(train_eng), max_length(train_spa)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYgiWRt1X1du",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model():\n",
        "  def __init__(self, encoder, decoder, source_tokenizer, target_tokenizer, source_max_length, target_max_length, model_name, rnn_size):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.source_tokenizer = source_tokenizer\n",
        "    self.target_tokenizer = target_tokenizer\n",
        "    self.source_max_length = source_max_length\n",
        "    self.target_max_length = target_max_length\n",
        "    self.rnn_size = rnn_size\n",
        "    # Initilize the optimizer:\n",
        "    self.optimizer = tf.keras.optimizers.Adam()\n",
        "    # Create checkpoint:\n",
        "#     checkpoint_dir = './training_checkpoints'\n",
        "#     self.checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_\"+model_name)\n",
        "#     self.checkpoint = tf.train.Checkpoint(optimizer=self.optimizer,\n",
        "#                                      encoder=self.encoder,\n",
        "#                                      decoder=self.decoder)\n",
        "\n",
        "    # Initilize the loss function\n",
        "    self.crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "    \n",
        "  @tf.function\n",
        "  def train_step(self, source, target, enc_hidden):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      enc_output, enc_hidden = self.encoder(source, enc_hidden)\n",
        "      dec_hidden = enc_hidden\n",
        "      dec_input = tf.expand_dims([self.target_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "      # Teacher forcing - feeding the target as the next input\n",
        "      for t in range(1, target.shape[1]):\n",
        "        # passing enc_output to the decoder\n",
        "        predictions, dec_hidden, _ = self.decoder(dec_input, dec_hidden, enc_output)\n",
        "        loss += self.calc_loss(target[:, t], predictions)\n",
        "        # using teacher forcing\n",
        "        dec_input = tf.expand_dims(target[:, t], 1)\n",
        "    batch_loss = (loss / int(target.shape[1]))\n",
        "    variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss\n",
        "\n",
        "  def calc_loss(self, targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    return self.crossentropy(targets, logits, sample_weight=mask)\n",
        "  \n",
        "  def train(self, source_train, target_train, epochs, batch_size):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((source_train, target_train))\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "    # calc steps per epoch\n",
        "    steps_per_epoch = len(source_train)//batch_size\n",
        "    for epoch in range(epochs):\n",
        "      start = time.time()\n",
        "\n",
        "      enc_hidden = self.encoder.init_state()\n",
        "      total_loss = 0\n",
        " \n",
        "\n",
        "      for (batch, (source, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = self.train_step(source, target, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                         batch,\n",
        "                                                         batch_loss.numpy()))\n",
        "     \n",
        "\n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                          total_loss / steps_per_epoch))\n",
        "      print('Time taken for 1 epoch {} sec'.format(time.time() - start))\n",
        "     \n",
        " \n",
        "    \n",
        "  def evaluate(self, sentence):\n",
        "    inputs = tf.expand_dims(sentence, axis=0)\n",
        "    result = ''\n",
        "    #print(inputs)\n",
        "    hidden = [tf.zeros((1, self.rnn_size))]\n",
        "    #print('1')\n",
        "    enc_out, enc_hidden = self.encoder(inputs, hidden)\n",
        "    #print('2')\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([self.target_tokenizer.word_index['<start>']], 0)\n",
        "    #print('3')\n",
        "    for t in range(self.target_max_length):\n",
        "        predictions, dec_hidden, attention_weights = self.decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "        \n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        #attention_plot[t] = attention_weights.numpy()\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += self.target_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "        if self.target_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence\n",
        "    \n",
        "        \n",
        "  def translate(self, sentence):\n",
        "    #print('Input: %s' % (sentence))\n",
        "    result, sentence = self.evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    #attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    #self.plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OFM0YzLvKrZ",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. From Eng to Spa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75CkLReMnBh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_spa_encoder = Encoder(eng_vocab_size, EMBEDDING_DIM, RNN_SIZE, BATCH_SIZE)\n",
        "eng_spa_decoder = Decoder(spa_vocab_size, EMBEDDING_DIM, RNN_SIZE, BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pHPtBzzX1mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "41a543d5-8eeb-493a-a74f-3c06b1c43960"
      },
      "source": [
        "eng_spa_model = Model(eng_spa_encoder, eng_spa_decoder, eng_tokenizer, spa_tokenizer, \"eng_spa\", max_length_eng, max_length_spa, RNN_SIZE)\n",
        "eng_spa_model.train(train_eng, train_spa, EPOCHS, BATCH_SIZE)#, test, spa_data_val)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.9999\n",
            "Epoch 1 Loss 1.7806\n",
            "Time taken for 1 epoch 58.503260135650635 sec\n",
            "Epoch 2 Batch 0 Loss 1.5379\n",
            "Epoch 2 Loss 1.5351\n",
            "Time taken for 1 epoch 3.4940223693847656 sec\n",
            "Epoch 3 Batch 0 Loss 1.4216\n",
            "Epoch 3 Loss 1.4554\n",
            "Time taken for 1 epoch 3.491806983947754 sec\n",
            "Epoch 4 Batch 0 Loss 1.3723\n",
            "Epoch 4 Loss 1.4152\n",
            "Time taken for 1 epoch 3.5640389919281006 sec\n",
            "Epoch 5 Batch 0 Loss 1.3354\n",
            "Epoch 5 Loss 1.3839\n",
            "Time taken for 1 epoch 3.4316999912261963 sec\n",
            "Epoch 6 Batch 0 Loss 1.3039\n",
            "Epoch 6 Loss 1.3595\n",
            "Time taken for 1 epoch 3.402456521987915 sec\n",
            "Epoch 7 Batch 0 Loss 1.2812\n",
            "Epoch 7 Loss 1.3409\n",
            "Time taken for 1 epoch 3.7065794467926025 sec\n",
            "Epoch 8 Batch 0 Loss 1.2695\n",
            "Epoch 8 Loss 1.3243\n",
            "Time taken for 1 epoch 4.243173122406006 sec\n",
            "Epoch 9 Batch 0 Loss 1.2543\n",
            "Epoch 9 Loss 1.3097\n",
            "Time taken for 1 epoch 4.253550291061401 sec\n",
            "Epoch 10 Batch 0 Loss 1.2425\n",
            "Epoch 10 Loss 1.2932\n",
            "Time taken for 1 epoch 3.564115524291992 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaT7mCyDa_Ig",
        "colab_type": "text"
      },
      "source": [
        "### 3.3. From Spa to Eng"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl-dMMzMX1tK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "2757017c-5992-4ec8-9c5c-1621ca0c02fd"
      },
      "source": [
        "spa_eng_encoder = Encoder(spa_vocab_size, EMBEDDING_DIM, RNN_SIZE, BATCH_SIZE)\n",
        "spa_eng_decoder = Decoder(eng_vocab_size, EMBEDDING_DIM, RNN_SIZE, BATCH_SIZE)\n",
        "spa_eng_model = Model(spa_eng_encoder, spa_eng_decoder, spa_tokenizer, eng_tokenizer, max_length_spa, max_length_eng, \"spa_eng\", RNN_SIZE)\n",
        "spa_eng_model.train(train_spa, train_eng, EPOCHS, BATCH_SIZE)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.4667\n",
            "Epoch 1 Loss 2.1850\n",
            "Time taken for 1 epoch 45.3548321723938 sec\n",
            "Epoch 2 Batch 0 Loss 1.9530\n",
            "Epoch 2 Loss 1.8963\n",
            "Time taken for 1 epoch 3.276911735534668 sec\n",
            "Epoch 3 Batch 0 Loss 1.7959\n",
            "Epoch 3 Loss 1.7981\n",
            "Time taken for 1 epoch 3.5957908630371094 sec\n",
            "Epoch 4 Batch 0 Loss 1.7317\n",
            "Epoch 4 Loss 1.7503\n",
            "Time taken for 1 epoch 3.5935492515563965 sec\n",
            "Epoch 5 Batch 0 Loss 1.6906\n",
            "Epoch 5 Loss 1.7078\n",
            "Time taken for 1 epoch 3.1771514415740967 sec\n",
            "Epoch 6 Batch 0 Loss 1.6470\n",
            "Epoch 6 Loss 1.6697\n",
            "Time taken for 1 epoch 2.898642063140869 sec\n",
            "Epoch 7 Batch 0 Loss 1.6098\n",
            "Epoch 7 Loss 1.6394\n",
            "Time taken for 1 epoch 2.93833327293396 sec\n",
            "Epoch 8 Batch 0 Loss 1.5809\n",
            "Epoch 8 Loss 1.6125\n",
            "Time taken for 1 epoch 2.9686360359191895 sec\n",
            "Epoch 9 Batch 0 Loss 1.5571\n",
            "Epoch 9 Loss 1.5874\n",
            "Time taken for 1 epoch 3.618291139602661 sec\n",
            "Epoch 10 Batch 0 Loss 1.5321\n",
            "Epoch 10 Loss 1.5633\n",
            "Time taken for 1 epoch 3.5522220134735107 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0pGNH_3X1z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCCopMT_bcFz",
        "colab_type": "text"
      },
      "source": [
        "### 3.4. Back translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LqCz9sAdOBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = shuffled_data[selected_id[ntrain:], :]\n",
        "test_data = [(preprocess(eng), preprocess(spa)) for (eng, spa) in test_data]\n",
        "test_eng, test_spa = list(zip(*test_data))\n",
        "test_eng = eng_tokenizer.texts_to_sequences(test_eng)\n",
        "test_eng = tf.keras.preprocessing.sequence.pad_sequences(test_eng, padding='post')\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTGqGkURftvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "72f153cf-53f3-489f-9421-9ab6326037b8"
      },
      "source": [
        "middle_spa = []\n",
        "origin_eng = []\n",
        "for i in range(ntest):\n",
        "  translation, sentence = eng_spa_model.evaluate(test_eng[i])\n",
        "  #print(input_sent)\n",
        "  origin_eng.append(test_data[i][0])\n",
        "  middle_spa.append(translation[:-5])\n",
        "  \n",
        "input_spa = spa_tokenizer.texts_to_sequences(middle_spa)\n",
        "input_spa = tf.keras.preprocessing.sequence.pad_sequences(input_spa, padding='post')\n",
        "#print(input_spa)\n",
        "\n",
        "back_eng = []\n",
        "for i in range(ntest):\n",
        "  translation, sentence = spa_eng_model.evaluate(input_spa[i])\n",
        "  #print(translation)\n",
        "  back_eng.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(back_eng, [origin_eng])\n",
        "print(results)  "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU(score=0.0, counts=[294, 87, 0, 0], totals=[500, 400, 300, 200], precisions=[58.8, 21.75, 0.0, 0.0], bp=0.4033300780671188, sys_len=500, ref_len=954)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rixJACShurId",
        "colab_type": "text"
      },
      "source": [
        "# Step 4. Conclusion\n",
        "In this assignment, I implement Bahdanau Attention mechanism in the translation model. \n",
        "Compared to the model without attention mechanism, one needs to modify the decoder and training process with \"teaching\".\n",
        "Because of the limited training and testint set, the BLEU didn't get better with the Bahdanau attention."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqVb3qw3fW6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6z9rFZWfW2j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKv6vyTVX0JB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqSWEDDzfKPq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}