{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note-Lec2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "clyKDmZDp0RB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TOPICS\n",
        "* Bias in data/incidents in DL\n",
        "* Linear classifiers\n",
        "* Softmax, loss\n",
        "* Lab"
      ]
    },
    {
      "metadata": {
        "id": "PCo5UrbRrvwe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Canary\n",
        "在进行正式的model之前，先小规模试验"
      ]
    },
    {
      "metadata": {
        "id": "IYa6KLFrthwT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DNN\n",
        "* Score function (forward pass)\n",
        "* Loss function\n",
        "* Update function (backward pass)"
      ]
    },
    {
      "metadata": {
        "id": "qLZJGoCVuD7G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Weights \\* Inputs + Bias = **Scores**\n",
        "\n",
        "**Epoch**: An 'epoch' is one complete pass, which means we've used each example once -> forward + backward.\n",
        "\n",
        "**Batch**: a non-overlapping chunk of the data"
      ]
    },
    {
      "metadata": {
        "id": "cgEiiL6p0cbr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Activation.\n",
        "### ReLU\n",
        "中间层activation 用ReLU.\n",
        "\n",
        "用sigmoid在gradient based learning时的坏处：当$|z|$变大时，sigmoid的导数趋向于0，不利于learning, 特别在DNN中（此时我们是将一堆小的gradients相乘）。<br>\n",
        "而ReLU算起来就快得多了，导数要么是1（z>0），要么是0（z<0）.\n",
        "\n",
        "### Softmax\n",
        "对于classification最后层，通常用softmax (multi class)。\n",
        "softmax converts scores to probabilities.\n",
        "Normalize each outpout to 0<x<1, and they sum to 1.\n",
        "\n",
        "$\\mathrm{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$.\n",
        "\n",
        "之所以用exp，是因为这样所有的prediction都>0.\n",
        "\n",
        "在code里：<br>\n",
        "softmax = np.exp(scores) / np.sum(np.exp(scores))\n",
        "\n",
        "问题是，higher scores 会变得特别大。"
      ]
    },
    {
      "metadata": {
        "id": "Y19xR3rs2RD7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss\n",
        "* 介于0（perfect）和正无穷（坏极了）之间的数，越小越好。\n",
        "* 衡量predicted probability distribution 以及target probability distribution之间的差异。\n",
        "* Cross entropy loss: 用于classfication。<br>\n",
        "$L = -\\sum_i \\hat{y_i} \\log (y_i)$，这其中$L$是cross entropy loss for a batch of examples, $\\hat{y_i}$是true prob, $y_i$是predicted prob.\n",
        "* 当softmax output on the correct example approaches 1, loss approaches 0, reaches a minimum; 当softmax output on the correct example approaches 0, loss reaches +inf, reaches a maximum.\n",
        "* 显然，对于randomly initialize weights, average loss是loss = -ln(1/n_classes).\n",
        "\n",
        "**为什么用cross entropy 而不是prediction error作为loss：**\n",
        "我们希望incorrect predictions be penalized more!\n",
        "各种error 并不equivalent.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rODUl4oG6wm_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 用MNIST实现上述过程"
      ]
    },
    {
      "metadata": {
        "id": "k8bkLaj1pwgi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "# Handwritten Digit Recognition dataset\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O-ki3L7W6Jom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "input_dim = 28*28\n",
        "x_train = x_train.reshape((60000, input_dim)) #[60000, 784]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkrrqGgy6mJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Initialize weights and bias\n",
        "hidden_size = 100\n",
        "output_classes = 10\n",
        "\n",
        "# Weights from input layer -> hidden layer\n",
        "W = 0.01 * np.random.randn(input_dim, hidden_size) #[784,100]\n",
        "b = np.zeros((1, hidden_size)) #[1,100]\n",
        "\n",
        "# Weights from hidden layer -> output layer\n",
        "W2 = 0.01 * np.random.randn(hidden_size, output_classes) #[100,10]\n",
        "b2 = np.zeros((1, output_classes)) #[1,10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3rg8i3wg7Xnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Begin gradient descent loop\n",
        "epochs = 5\n",
        "num_examples = 60000\n",
        "for i in range(epochs):\n",
        "  #Forward propogate\n",
        "  hidden_layer = np.maximum(0, np.dot(x_train, W)+b) #ReLU [60000,100]\n",
        "  scores = np.dot(hidden_layer, W2) + b2 #[60000,10]\n",
        "  \n",
        "  #Softmax\n",
        "  exp_scores = np.exp(scores)\n",
        "  probs = exp_scores /np.sum(exp_scores, axis = 1, keepdims=True) #[60000,10]\n",
        "  \n",
        "  #Loss\n",
        "  losses = -np.log(probs[range(num_examples),y_train]) #[60000, 1]\n",
        "  loss = np.sum(losses)/num_examples #[1,]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}