{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lec3-notes-C4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "eVnRLGnh_Vha",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lec3 Reading要求：\n",
        "* DLwP: C4&5 (完成C4，C5在另一个文件里)\n",
        "* <font color='red'>(未完成) </font>DL: 9\n"
      ]
    },
    {
      "metadata": {
        "id": "2jss7aNROsQA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DLwP: C4, Fundamental Machine Learning"
      ]
    },
    {
      "metadata": {
        "id": "W-szFYy-LZNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Classification & regression glossary\n",
        "\n",
        "* Sample/input: one data point\n",
        "* Prediction/output\n",
        "* Target -- The truth\n",
        "* Prediction error/loss value: 衡量model's prediction 与target间差距。\n",
        "* Classes: A set of possible labels to choose from in classification\n",
        "* Label\n",
        "* Ground-truth/annotations: All targets for a dataset.\n",
        "* Binary classification\n",
        "* Mutliclass classification\n",
        "* Multilabel classification: 输出可能是多个label。比如图像识别，可能有的图中既有猫又有狗，那么就有两个label.\n",
        "* Scalar regression: 输出是单个的连续的scalar value.\n",
        "* Vector regression: 输出是set of continuous values, 比如a continuous vector.\n",
        "* Mini-batch/batch. 训练中梯度下降批量大小，通常是8～128间，并且是2的power.\n"
      ]
    },
    {
      "metadata": {
        "id": "5ke1SOnSMx2r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 数据量少，防止overfitting:\n",
        "* Hold-out: 取一部分training出来做validation\n",
        "* K-fold: training均分成k份，每次抽一份出来做validation, 剩下的train. 然后平均k次的结果.最后用所有training data 重新train model.\n",
        "* Iterated K-fold: 数据量特别少的时候，将上面的K-fold进行P次，每次划分前shuffle data. Performance好，expensive. \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 注意：\n",
        "* Data representativeness: training data应该足够代表population 性质。所以**randomly shuffle**很重要！\n",
        "* Arrow of time: 如果是预测将来的情况，就不能随意shuffle. 必须保证test set中的数据都在training set时间之后。\n",
        "* Redundancy in data: training和test中不应该有重复数据，两者应该无关。"
      ]
    },
    {
      "metadata": {
        "id": "ihzGuDBgSuEN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.3. Data preprocessing, feature engineering, and feature learning.\n",
        "### 1. Data Vectorization\n",
        "NN中所有输入和输出都是浮点类，所以所有的数据都必须编程tensor, 称为**Data Vectorization**.\n",
        "\n",
        "###  2. Value Normalization。\n",
        "需要data满足：\n",
        "* Take small values：通常都在0～1之间.\n",
        "* Be homogeneous：所有feature roughly in the same range.<br>\n",
        " 所以常见的处理是x=(x-mean)/std，从而保证mean=0, std=1.\n",
        " \n",
        "### 3. Missing Values\n",
        " 通常用0来代替missing values. NN 会learn到这些是missing values，可以ignore。\n",
        " \n",
        " **但是**，如果testing data中有missing values，而training data中没有，就会出现问题（NN不知道这些是missing values）。需要人为在training中制造\n",
        " \n",
        "### 4. Feature Engineering\n",
        "找到合适的feature representation\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "KbpEwlUmcEbM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4.4. Overfitting and underfitting\n",
        "最好的方式是获取更多的training data。\n",
        "但这不一定能实现。\n",
        "可以限制model存储的信息，这种防止overfitting的方式称为**regularization**.\n",
        "\n",
        "Model's **capacity**: number of learnable parameters in the model.\n",
        "\n",
        "需要在overfitting和underfitting之间权衡。通常是从小模型开始（更少的layer，更少的units per layer）, 通过validation loss 去调整，找到合适的model.\n",
        "\n",
        "除了上面这种人为选取simple model的方式，还可以进行weight regularization. 对loss function加上weight cost, 可以抑止大值weights. 常见两种：\n",
        "* L1 regularization: L1 norm, weight 绝对值。\n",
        "* L2 regularization: L2 norm, weight 平方。又称为weight decay.\n",
        "\n",
        "code如下：\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Uk6FO-mg_T_A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import regularizers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, kernel_regularizer=regularizers.l2(0.001),\n",
        "                      activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dBK3Cs2IhI9M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "l2(0.001) means every coefficient in the weight matrix of the layer will add 0.001 \\* weight_coefficient_value to the total loss of the network. Note that because this penalty is only added at training time, the loss for this network will be much higher at training than at test time.\n",
        "\n",
        "也可以同时加l1和l2: <br>\n",
        "regularizers.l1_l2(l1=0.001, l2=0.01)"
      ]
    },
    {
      "metadata": {
        "id": "tospmmQghoGI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "Dropout是最常用的regularization方法。\n",
        "training时，随机扔掉某一层中的一部分output features.\n",
        "所以下一层收到的这些feature对应的值就是0.\n",
        "\n",
        "dropout rate通常取在0.2～0.5之间。\n",
        "\n",
        "test 时，这些units不会drop out, 但是输出值会以dropout rate 来scale down, 去balance.因为test时没有dropout, 那么比training时有更多的active units.\n",
        "\n",
        "<font color='red'>这里讲test中scale有点迷糊，觉得可能有差？</font> \n",
        "\n",
        "The **core idea** is that introducing noise in the output values of a layer can break up happenstance patterns that aren’t significant, which the network will start memorizing if no noise is present.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mAS16_BAjez5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(layers.Dropout(0.5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5C5B9CTkkS3o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The most common ways to prevent overfitting in neural networks:\n",
        "* Get more training data.\n",
        "* Reduce the capacity of the network. \n",
        "* Add weight regularization.\n",
        "* Add dropout."
      ]
    },
    {
      "metadata": {
        "id": "sn0Chq76m36M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 调试model,看overfitting边界\n",
        "* Add layers.\n",
        "* Make the layers bigger.\n",
        "* Train for more epochs.<br>\n",
        "看 training loss and validation loss, 或其它metrics.\n",
        "若validation的performance下降，说明overfitting.\n",
        "\n",
        "\n",
        "### Regularizing and tuning hyperparameters.\n",
        "* Add dropout\n",
        "* Try different architectures: 加减层\n",
        "* Add L1/L2 regularization\n",
        "* 试不同hyperparameters.\n",
        "* 可以试试feature engineering.\n",
        "\n",
        "注意对validation 也不能太多次处理，否则也可能overfit to validation data. "
      ]
    },
    {
      "metadata": {
        "id": "jO7fbQuuw58Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Chapter summary\n",
        "* Define the problem at hand and the data on which you’ll train. Collect this data, or annotate it with labels if need be.\n",
        "* Choose how you’ll measure success on your problem. Which metrics will you monitor on your validation data?\n",
        "* Determine your evaluation protocol: hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
        "* Develop a first model that does better than a basic baseline: a model with statistical power.\n",
        "* Develop a model that overfits.\n",
        "* Regularize your model and tune its hyperparameters, based on performance on the validation data. A lot of machine-learning research tends to focus only on this step—but keep the big picture in mind."
      ]
    }
  ]
}