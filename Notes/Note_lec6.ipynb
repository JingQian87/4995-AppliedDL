{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note-lec6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "3y411PC53ThJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TOPICS: \n",
        "1. Concepts:\n",
        " * Dropout\n",
        " * Embeddings and the embedding projector\n",
        "2. TensorFlow (可见assignment3)\n",
        " * TensorBoard\n",
        " * TF1, TF2 (tf.function and AutoGraph)"
      ]
    },
    {
      "metadata": {
        "id": "51Y2tDjg4AQm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Dropout\n",
        "## 使用\n",
        "* Drop neurons in **hidden** layers\n",
        "* **Dropout rate** is the fraction of the activations that are zeroed out; it’s usually set between 0.2 and 0.5 \n",
        "* No activations are dropped at testing time\n",
        "\n",
        "## 作用：\n",
        "* Forces the model to learn redundant representations / reduces it’s capacity / can only “remember” most important patterns. \n",
        "* A little bit like ensembling (we’re loosely training a different model on each iteration).\n",
        "\n",
        "## 方法\n",
        "* Common pattern: use following Dense or Flatten layers.\n",
        "* How many Dropout layers should you use? Hyperparameter. <br>\n",
        "Best bet: start with just one, right before the output layer.\n",
        "* If you’re using Dropout with the Subclassing API, you will need to pass a parameter to let your model know whether it’s train or test time.\n"
      ]
    },
    {
      "metadata": {
        "id": "m6vlPSnf5Tlv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Embedding\n",
        "\n",
        "## One-hot encodings\n",
        "results in sparse, high-dimensional vector.\n",
        " 比如5在one-hot encodings中就是[0.0,0,0,0,1,0,0,0,0]，对应位置是0～9.\n",
        " \n",
        " Notes: the embedding layer is basically a lookup table. Embedding weights begin randomly and are adjusted by the classifier by backprop (exactly as in a Dense layer). Size of the embedding is a hyperparameter. How is the lookup performed? Using a dictionary mapping from integer-encoded words -> embeddings. This is more efficient than an approach you may see written in math notation (where a one-hot vector is multiplied against a matrix to “select” a column).\n",
        " \n",
        "Embeddings: Dense, lower-dimensional vectors learned from data.<br>\n",
        "**Common size**s: 8 (if training your a small amount of data) to 1024 (if training a large model for reuse down the road).\n",
        " \n",
        " <font color='red'>这里不太懂，大概意思是像翻译一样，把feature处理变成input。比如单词变成数字，又比如单词的不同形式可以投影到同一个值 之类的？</font>\n",
        " \n",
        " https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/sequences/word_embeddings.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "O-LA0UoU7mCE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF2\n",
        "Graph\n",
        "\n",
        "尽量用tf.\\* 而不是用np.\\*，后者比较低效"
      ]
    },
    {
      "metadata": {
        "id": "_4OhmONT3PXW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}