{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reading-Lec1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "U2spP-LnvunG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# L1 Reading要求：\n",
        "* DLwP: C1&2 （已完成）\n",
        "* <font color='red'>未完成</font> DL: C1 \n",
        "* <font color='red'>未完成</font> Python DS Handbook\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "i9ZgBaKdTjMG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# DLwP C1\n",
        "## 1.1\n",
        "**Machine learning**: find useful representations of input data. 各种变形，比如linear projections, translations, nonlinear operations, etc.\n",
        "\n",
        "** Deep Learning**, 又名layered representations learning, hierarchical representations learning.\n",
        "* a multistage way to learn data representations\n",
        "* depth = \\# layers in the model\n",
        "* 每一层的weights决定了transformation 如何参数化。我们需要找到合适的weights使得input map correctly to targets.\n",
        "* 衡量方式：loss function/ objective function, compute distance between prediction & true value. 得到*loss score*.\n",
        "* Optimizer: 通过backpropogation 去调整weights, 从而降低loss score.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "D5TkAWPSXbBC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1.2. Brief history of ML\n",
        "1. Probabilistic Modeling. \n",
        "Eg. Naive Bayes, Logistic regression.\n",
        "\n",
        "2. Kernel Methods.\n",
        "Eg: SVM\n",
        "\n",
        "3. Decision Tree. Random Forest. Gradient Boosting Machines-> 最好的适合nonperceptual data的算法\n",
        "\n",
        "DL优点：不用烦feature engineering.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UDCsvdUszEA4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "# DLwP C2"
      ]
    },
    {
      "metadata": {
        "id": "XD35EpPKRuk7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1. MNIST example\n"
      ]
    },
    {
      "metadata": {
        "id": "NC-6pxdAUgGF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1. Load data"
      ]
    },
    {
      "metadata": {
        "id": "TFIKVE-oSb1e",
        "colab_type": "code",
        "outputId": "96ac555f-fa32-4949-f3e1-4770c9c191e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SzBTetHKvfWP",
        "colab_type": "code",
        "outputId": "8cb2d190-3c47-4148-81dd-4ea71fab6a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a1L8Kl8TSxGp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "train set 有60000 samples, test set 有10000 samples. \n",
        "每个图像为 28x28 的 NumPy 数组，像素值介于 0 到 255 之间。标签是整数数组，介于 0 到 9 之间。\n"
      ]
    },
    {
      "metadata": {
        "id": "JJ-tRjLwUjDf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2. Network"
      ]
    },
    {
      "metadata": {
        "id": "Iz1qob6Q06ZX",
        "colab_type": "code",
        "outputId": "b687f8b0-5b0b-4faf-c120-7d340b42e1b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "network = models.Sequential()\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28*28,)))\n",
        "network.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zVXUXcaZVCcB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Layer**: Data processing module, 也可视为filter for data. <br>\n",
        "Layer可以提取输入的data中的representation。\n",
        "通常deep learning都是由一堆简单的layer堆叠而成，成为data distillation.\n",
        "\n",
        "在上面的命令中，network由两层Dense layer组成, 是densely connected (又称为fully connected) neural layers。\n",
        "第二层是10-way softmax layer, 会输出10个probability scores (sum=1)。每个score表示图像是属于对应的digit的概率。\n"
      ]
    },
    {
      "metadata": {
        "id": "B1a7cShMa5Qn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 3. Compilation \n",
        "* **Loss Function**: 衡量network在training data上的performance。并可以据此steer itself in the right direction\n",
        "* **Optimizer**: 据此network update itself\n",
        "* **Metrics to monitor during training and testing**: 这里我们只关心accuracy (fraction = #correctly classified/#all).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IexZ-Cwwbndm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "network.compile(optimizer='rmsprop',\n",
        "               loss='categorical_crossentropy',\n",
        "               metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qFVphDwmb8Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 4. Preparing the image data\n",
        "我们知道，输入的training images，shape是(60000, 28, 28)。每个像素值在[0, 255]之间，类型是uint8。但我们想把它们变成[0, 1]之间的float 32。所以："
      ]
    },
    {
      "metadata": {
        "id": "M7u53kl5dDmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_images = train_images.reshape((60000, 28*28))\n",
        "train_images = train_images.astype('float32')/255\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UTUfyC4PdhUC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<font color='red'>其实在老师给的例子里也没有这么麻烦，直接train_images=train_images/255.0，也没有把labels变成categorical。但这里不行，可能是因为那里直接从tf.keras调用？看看后面再说</font>\n",
        "\n",
        "解决了，老师给的例子里是先除以255.0，就变成了float; 然后flatten,就是reshape了。 "
      ]
    },
    {
      "metadata": {
        "id": "ymtK3d4veU_6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xWL88EIheflO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5. Fit data to model"
      ]
    },
    {
      "metadata": {
        "id": "jSAEUaobdv7F",
        "colab_type": "code",
        "outputId": "f0890101-f7b1-44d8-99e3-562486ec1b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "cell_type": "code",
      "source": [
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 98us/step - loss: 0.2530 - acc: 0.9269\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.1024 - acc: 0.9694\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0676 - acc: 0.9797\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.0492 - acc: 0.9855\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.0377 - acc: 0.9882\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f28c1541e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "7dUORSxQeix-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "上面给出了training中的两个数值： **loss** & **accuracy**."
      ]
    },
    {
      "metadata": {
        "id": "kCjESRVee1Kk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 6. Test fitted model"
      ]
    },
    {
      "metadata": {
        "id": "qzZrPNFle5Cy",
        "colab_type": "code",
        "outputId": "8f8dd6b3-e24b-49b5-c162-53896648e73d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print(test_loss, test_acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 53us/step\n",
            "0.06998087865991984 0.9781\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7vEUD5odfDO-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2. Data representations for neural networks"
      ]
    },
    {
      "metadata": {
        "id": "8kgXmcDJfSKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. 讲了tensor的定义，ndim等等。\n",
        "\n",
        "2. batch。把数据分成一部分一部分的去train. 按第一个轴（对应sample 编号）划分，所以first axis (axis 0) is called the batch axis or batch dimension\n",
        "\n",
        "3. 常见的data tensors:\n",
        "> * Vector data: 2D tensors (samples, features)\n",
        "> * Timeseries data (sequence data): 3D tensors (samples, timesteps, features)\n",
        "> * Images: 4D tensors (samples, height, width, channels)或者(samples, channels, height, width)\n",
        "> * Video: 5D tensors (samples, frames, height, width, channels) 或者(samples, frames, channels, height, width)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jVh_Q-TGnkt0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3. Tensor operations"
      ]
    },
    {
      "metadata": {
        "id": "YB92muWNnqyD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. element-wise operation\n",
        "\n",
        "2. Broadcasting. 不同shape的tensors间的操作，输出的ndim是输入中ndim更大的那个。<br>\n",
        " Broadcasting consists of two steps: <br>\n",
        "1) Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.<br>\n",
        "2) The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.\n",
        "\n",
        "3. Tensor dot, 也称为tensor product. 就是点乘，比如矢量间点乘为标量；矩阵点乘矢量为矢量。\n",
        "\n",
        "4. Tensor reshaping\n"
      ]
    },
    {
      "metadata": {
        "id": "PkEOt5ag7wwK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.4. Gradient-based Optimization\n",
        "\n",
        "### 1. Training loop\n",
        "随机、小值初始化weights (kernal&bias), 通过forward, 用feedback signal 逐渐调整weights:\n",
        "* 提取training x & y batch.\n",
        "* 在x上run network (forward pass)， 得到y_pred.\n",
        "* 比较network loss, 即y与y_pred间差异\n",
        "* 更新网络所有weights, 从而减小这个batch的loss.\n",
        "\n",
        "最后一步的做法：考虑到网络中所有操作都是可微分的，计算**gradient** of the loss with regard to the network's coefiicients.\n",
        "\n",
        "### 2. 一些取值：\n",
        "* Step factor: <br>\n",
        "不能太小，否则可能stuck in a local minimum. 不能太大， 否则可能end up taking you to completely random locations on the curve.\n",
        "\n",
        "* Batch size: <br>\n",
        "在特别小的mini-batch SGD 和整体的batch SGD中找到平衡。\n",
        "\n",
        "### 3. Optimizers: \n",
        "比如Adam, RMSProp, SGD....\n",
        "带momentum的GD就不担心掉到local minimum里。\n",
        "\n",
        "Momentum is implemented by moving the ball at each step based not only on the current slope value (current acceleration) but also on the current velocity (resulting from past acceleration). In practice, this means updating the parameter w based not only on the current gradient value but also on the previous parameter update.\n",
        "\n",
        "### 4. Backpropagation\n",
        "又称为reverse-mode differentiation.\n",
        "用到chain rule\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "x6NOFhRES9H9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.5.\n",
        "Epoch: each iteration over all the training data is called an epoch\n",
        "\n",
        "对于开始的例子，60000个samples, 每个batch包括128个sample, 总共5个epoch. 所以参数更新了5x(60000/128) = 2345次。\n"
      ]
    },
    {
      "metadata": {
        "id": "hi-6f4ZqT8p2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Chapter Summary:\n",
        "* **Learning** means finding a combination of model parameters that minimizes a loss function for a given set of training data samples and their corresponding targets.\n",
        "* Learning happens by drawing random batches of data samples and their targets, and computing the gradient of the network parameters with respect to the loss on the batch. The network parameters are then moved a bit (the magnitude of the move is defined by the learning rate) in the opposite direction from the gradient.\n",
        "* The entire learning process is made possible by the fact that neural networks are chains of differentiable tensor operations, and thus it’s possible to apply the chain rule of derivation to find the gradient function map- ping the current parameters and current batch of data to a gradient value.\n",
        "* Two key concepts you’ll see frequently in future chapters are **loss and optimizers**. These are the two things you need to define before you begin feeding data into a network.\n",
        "* The **loss** is the quantity you’ll attempt to minimize during training, so it should represent a measure of success for the task you’re trying to solve.\n",
        "* The optimizer specifies the exact way in which the gradient of the loss will be used to update parameters: for instance, it could be the RMSProp optimizer, SGD with momentum, and so on."
      ]
    }
  ]
}