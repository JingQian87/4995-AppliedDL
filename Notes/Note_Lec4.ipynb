{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note-Lec4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "clyKDmZDp0RB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TOPICS: \n",
        "* Numerical stability\n",
        "* Gradient descent basics\n",
        "* Backpropagation basics\n",
        "\n",
        "<font color='red'>不相关，还没看</font>: \n",
        "* do classifier on structured data:\n",
        "https://colab.research.google.com/github/random-forests/applied-dl/blob/master/examples/feature_cols.ipynb\n",
        "* GAN:\n",
        "https://colab.research.google.com/github/random-forests/docs/blob/master/site/en/r2/tutorials/generative/dcgan.ipynb"
      ]
    },
    {
      "metadata": {
        "id": "fMRb03yGPVNr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Numerical Stability (overflow & underflow)\n",
        "\n",
        " Underflow: Numbers near zero are rounded to zero\n",
        " \n",
        " Overflow: Large finite numbers are approximated as infinity.\n",
        " 可能后来会导致NAN.\n",
        " \n",
        " **Softmax**\n",
        " \n",
        " **cross entropy**\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "gQk6V_chvxXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Clipping\n",
        "\n",
        " softmax_output = tf.clip_by_value(softmax_output, _epsilon, 1. - _epsilon)\n",
        " \n",
        " Guarantees all values in softmax output are 0 < x < 1 before computing cross entropy, so we never attempt log(0).\n",
        " \n",
        " This technique may feel a bit simplistic (by that I mean - our field is young, and perhaps there’s a better way yet to be discovered!) A similar trick is used to deal with exploding gradients.\n",
        " \n",
        " <font color='red'>P25有为什么不能直接计算cross entropy的问题,再看看。在程序里是softmax&cross entropy打包的</font>"
      ]
    },
    {
      "metadata": {
        "id": "Q5gOSrArwvZ1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Vanishing gradients:\n",
        "一堆小于1的数相乘，最后结果会趋近于0.\n",
        "\n",
        "### Nomalize input data"
      ]
    },
    {
      "metadata": {
        "id": "D1RA8jDAxQdG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent\n",
        "\n",
        " Two sources of complexity\n",
        "1. The gradient descent algorithm itself momentum, or adaptive learning rates).\n",
        "2. The method of computing the gradients themselves (backprop).\n",
        " \n",
        "**Adam**: default\n",
        "\n",
        "Stochastic, batch, mini-batch. <br>\n",
        "mini-batch可以parallel training\n",
        "\n",
        "**Why do models have more parameters than necessary?**<br>\n",
        "Gradient descent在network不大，特别是正好的时候不可信，更容易掉到local minimum里。\n",
        "所以我们要更大的network, 再做regularization防止overfit.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-gZHXSDd1DfE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Backprop\n",
        "\n",
        "* Backprop is a method to efficiently calculate gradients by recursive application of the chain rule on a computation graph.\n",
        "* The key is to realize each node on the graph can calculate its local gradient independently (it doesn’t need to know anything about the structure of the graph).\n",
        "\n",
        "<font color='red'>还需要理解下怎么实现的:\n",
        "https://github.com/random-forests/applied-dl/blob/master/examples/3.1-backprop.ipynb\n",
        "</font>"
      ]
    }
  ]
}