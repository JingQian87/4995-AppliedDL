{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note_DLwP_C6_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2vLv8F93gx3P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Text and Sequences"
      ]
    },
    {
      "metadata": {
        "id": "LHjfnyycg4_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.1. Working with text data\n",
        "这里是sequence data，这其中text是最广泛的。\n",
        "可以理解为sequence of characters 或sequene of words，但words层面最常用。\n",
        "\n",
        "**将text转化为numeric tensors:**\n",
        "* Each word -> A vector.\n",
        "* Each character -> A vector\n",
        "* Each n-grams of words/characters -> A vector.<br>\n",
        "N-grams是overlapping groups of multiple consecutive words or  characters.\n",
        "\n",
        "不同分解方式得到的unit称为tokens, 对应的分解称为tokenization.\n",
        "\n",
        "**两种常用的将vector与token联系起来的方式：**\n",
        "* One hot encoding\n",
        "* Token embedding (通常只对words这么做，所以称为word embedding)\n",
        "\n",
        "**Bag of words**\n",
        "* 将text划分成set of 2-grams（连续的一个或两个单词组成的gram,可以重叠），这样的set称为bag-of-2-grams.\n",
        "因为是set,所以没有顺序。\n",
        "* The family of tokenization methods is called bag-of-words.\n",
        "* Extracting n-grams 是feature engineering, 在非deep learning，比如logistic regression和random forests中很有用。\n",
        "* Deep learning中不用这个，我们用hierarchical feature learning."
      ]
    },
    {
      "metadata": {
        "id": "JvAFHPYkg7sz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding of words and characters\n",
        "将每个单词用一个独特的整数index表示，再把这个index化为binary vector of size N (N 是vocabulary的size). Vector中只有对应index的位置是1，其它是0.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nS5yQpQgg_eQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "-p7vzGaMhCFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Putting it all together: from raw text to word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "bAze1Lu_hGVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "JyKCDh6YhKuB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.2. Understanding recurrent neural networks"
      ]
    },
    {
      "metadata": {
        "id": "FA81weB4hLNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A recurrent layer in Keras"
      ]
    },
    {
      "metadata": {
        "id": "a0Mqd2khhLQD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding the LSTM and GRU layers"
      ]
    },
    {
      "metadata": {
        "id": "PF_9g87uhLSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A concrete LSTM example in Keras"
      ]
    },
    {
      "metadata": {
        "id": "Gu3a6_TshLVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "FPq5hJp4hLYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.3. Advanced use of recurrent neural networks"
      ]
    },
    {
      "metadata": {
        "id": "mLXRBLB6hLbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A temperature-forecasting problem"
      ]
    },
    {
      "metadata": {
        "id": "n1Jz73oehLd6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "metadata": {
        "id": "3xXZvo3_hLg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A common-sense, non-machine-learning baseline"
      ]
    },
    {
      "metadata": {
        "id": "HJG1papFhLkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A basic machine-learning approach"
      ]
    },
    {
      "metadata": {
        "id": "AfXayiiQhLnL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A first recurrent baseline"
      ]
    },
    {
      "metadata": {
        "id": "UOcNZ75Fhx-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using recurrent dropout to fight overfitting"
      ]
    },
    {
      "metadata": {
        "id": "Eyby7or-hyBs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stacking recurrent layers"
      ]
    },
    {
      "metadata": {
        "id": "SKYdbp9ihyEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using bidirectional RNNs"
      ]
    },
    {
      "metadata": {
        "id": "6x7ZpxufhyHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Going even further"
      ]
    },
    {
      "metadata": {
        "id": "IPVVlzE-hyKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "P2iMcd00hyM7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.4. Sequence processing with convnets"
      ]
    },
    {
      "metadata": {
        "id": "n9GkEtJMiEHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding 1D convolution for sequence data"
      ]
    },
    {
      "metadata": {
        "id": "u-CUQRVtiHni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1D pooling for sequence data"
      ]
    },
    {
      "metadata": {
        "id": "jT1p2v30iJqy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing a 1D convnet"
      ]
    },
    {
      "metadata": {
        "id": "ljC4CpZ0iMME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combing CNNs and RNNs to process long sequences"
      ]
    },
    {
      "metadata": {
        "id": "QxYz4UVhiMPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "O4P30oFPiMSW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter summary"
      ]
    },
    {
      "metadata": {
        "id": "q0-YNz-FhyP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}