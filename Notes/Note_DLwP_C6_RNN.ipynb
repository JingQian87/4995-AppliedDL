{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note_DLwP_C6_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2vLv8F93gx3P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep Learning for Text and Sequences"
      ]
    },
    {
      "metadata": {
        "id": "LHjfnyycg4_L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.1. Working with text data\n",
        "这里是sequence data，这其中text是最广泛的。\n",
        "可以理解为sequence of characters 或sequene of words，但words层面最常用。\n",
        "\n",
        "**将text转化为numeric tensors:**\n",
        "* Each word -> A vector.\n",
        "* Each character -> A vector\n",
        "* Each n-grams of words/characters -> A vector.<br>\n",
        "N-grams是overlapping groups of multiple consecutive words or  characters.\n",
        "\n",
        "不同分解方式得到的unit称为tokens, 对应的分解称为tokenization.\n",
        "\n",
        "**两种常用的将vector与token联系起来的方式：**\n",
        "* One hot encoding\n",
        "* Token embedding (通常只对words这么做，所以称为word embedding)\n",
        "\n",
        "**Bag of words**\n",
        "* 将text划分成set of 2-grams（连续的一个或两个单词组成的gram,可以重叠），这样的set称为bag-of-2-grams.\n",
        "因为是set,所以没有顺序。\n",
        "* The family of tokenization methods is called bag-of-words.\n",
        "* Extracting n-grams 是feature engineering, 在非deep learning，比如logistic regression和random forests中很有用。\n",
        "* Deep learning中不用这个，我们用hierarchical feature learning."
      ]
    },
    {
      "metadata": {
        "id": "JvAFHPYkg7sz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## One-hot encoding of words and characters\n",
        "将每个单词用一个独特的整数index表示，再把这个index化为binary vector of size N (N 是vocabulary的size). Vector中只有对应index的位置是1，其它是0.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "QCAOh7KgzOLO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "47e41182-b8b2-4c73-e64e-dd92a4b68560"
      },
      "cell_type": "code",
      "source": [
        "# Keras built-in for one-hot encoding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "#only take into account the 1000 most common words\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' %len(word_index))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 9 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kmBt2OYAzNDL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**One-hot hashing trick**: \n",
        "* 当vocabulary too large, 不精确把每个单词assign index，而是hash words into vectors of fixed size. \n",
        "* 对online encoding 好\n",
        "* 问题是可能产生hash collisions.\n",
        "* 适用于dimensionality of the hashing space is much larger than the total number of unique tokens being hashed."
      ]
    },
    {
      "metadata": {
        "id": "nS5yQpQgg_eQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "KRZu-0Cp1i94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "one-hot encoding太占空间了，特别是词汇量多的时候。\n",
        "比较：\n",
        "\n",
        "<h4 style='padding: 10px'>One-hot word vectors与word embedding差别</h2><table class='table table-striped'> <thead> <tr> <th> </th> <th>One-hot word vectors</th> <th>Word embeddings</th>  </tr> </thead> <tbody> <tr> <th scope='row'>1</th> <td>Sparse</td> <td>Dense</td> </tr> <tr> <th scope='row'>2</th> <td>High-demensional</td> <td>Lower-dimensional</td> </tr> <tr> <th scope='row'>3</th> <td>Hardcoded</td> <td>Learned from data</td> </tbody> </table>\n",
        "\n",
        "可以通过NN learn embedding,或者借用别的\n"
      ]
    },
    {
      "metadata": {
        "id": "odJVk01L2hcl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Learn word embeddings with the embedding layer.\n",
        "**Embedding layer:** A dictionary that maps integer indices (which stand for specific words) to dense vectors. <br>\n",
        "Word index -> Embedding layer -> Corresponding word vector\n",
        "\n",
        "与其它层一样，randomized initialization.在training过程中，通过backpropogation gradually adjusted.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9Pnn5WZsA9ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3290b66a-4573-4024-93d2-8a79ce12d63d"
      },
      "cell_type": "code",
      "source": [
        "# Loading the IMDB data\n",
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words = max_features)\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen = maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test,maxlen = maxlen)\n",
        "#只选取每个sample前maxlen个单词"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aBw6WPNGCvnQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 717
        },
        "outputId": "223ae71a-09ef-4ab5-f189-f38390b438da"
      },
      "cell_type": "code",
      "source": [
        "# Using an Embedding layer and classifier on the IMDB data\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                   epochs=10,\n",
        "                   batch_size=32,\n",
        "                   validation_split=0.2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 161       \n",
            "=================================================================\n",
            "Total params: 80,161\n",
            "Trainable params: 80,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 2s 96us/step - loss: 0.6759 - acc: 0.6043 - val_loss: 0.6398 - val_acc: 0.6810\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 1s 61us/step - loss: 0.5657 - acc: 0.7428 - val_loss: 0.5467 - val_acc: 0.7206\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.4263 - acc: 0.8079 - val_loss: 0.5008 - val_acc: 0.7454\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 1s 56us/step - loss: 0.3930 - acc: 0.8257 - val_loss: 0.4981 - val_acc: 0.7540\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.5013 - val_acc: 0.7534\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.3435 - acc: 0.8534 - val_loss: 0.5051 - val_acc: 0.7518\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 1s 57us/step - loss: 0.3223 - acc: 0.8658 - val_loss: 0.5132 - val_acc: 0.7486\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 1s 58us/step - loss: 0.3022 - acc: 0.8765 - val_loss: 0.5213 - val_acc: 0.7492\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 1s 57us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7466\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IKcD2EnmBDq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "两个常用的precomputed databases of word embeddings: Word2Vec, GloVe (called Global Vectors)"
      ]
    },
    {
      "metadata": {
        "id": "-p7vzGaMhCFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Putting it all together: from raw text to word embeddings"
      ]
    },
    {
      "metadata": {
        "id": "ws8AOE-pHzYq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using pretrained word embeddings\n",
        "\n",
        "<font color='red'>未完成！！！！</font>"
      ]
    },
    {
      "metadata": {
        "id": "bAze1Lu_hGVg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up\n",
        "本节内容：\n",
        "* 将text转化为NN可以处理的数据类型。\n",
        "* 两种embedding 方式：\n",
        "  * Task-specific token embeddings, 用Keras里的Embedding layer。\n",
        "  * 用 pretrained word embeddings，解决小型NLP问题。 "
      ]
    },
    {
      "metadata": {
        "id": "JyKCDh6YhKuB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.2. Understanding recurrent neural networks\n"
      ]
    },
    {
      "metadata": {
        "id": "Zs1WJ8MDKNhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "原理：maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in.\n",
        "\n",
        "recurrent neural network (RNN)： processes sequences by iterating through the sequence elements and maintaining a state containing information relative to what it has seen so far.\n",
        "\n",
        "在给的numpy样本里，他把previous output + current input结合起来作为当下的input (W 和U是两个parameter matrices)："
      ]
    },
    {
      "metadata": {
        "id": "zRoQLiXqNhVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "state_t = 0\n",
        "for input_t in input_sequence:\n",
        "  output_t = activation(dot(W, input_t) + dot(U, state_t) + b)\n",
        "  state_t = output_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IM6FhqabP5fR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Final output是2D tensor of shape (timesteps, output_features), which each timestep is the output of the loop at time t. 最后的输出反映了整个sequence的信息。"
      ]
    },
    {
      "metadata": {
        "id": "FA81weB4hLNL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A recurrent layer in Keras"
      ]
    },
    {
      "metadata": {
        "id": "pz4aielyQV4M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import SimpleRNN\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Em)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a0Mqd2khhLQD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding the LSTM and GRU layers"
      ]
    },
    {
      "metadata": {
        "id": "PF_9g87uhLSz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A concrete LSTM example in Keras"
      ]
    },
    {
      "metadata": {
        "id": "Gu3a6_TshLVo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "FPq5hJp4hLYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.3. Advanced use of recurrent neural networks"
      ]
    },
    {
      "metadata": {
        "id": "mLXRBLB6hLbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A temperature-forecasting problem"
      ]
    },
    {
      "metadata": {
        "id": "n1Jz73oehLd6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "metadata": {
        "id": "3xXZvo3_hLg5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A common-sense, non-machine-learning baseline"
      ]
    },
    {
      "metadata": {
        "id": "HJG1papFhLkB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A basic machine-learning approach"
      ]
    },
    {
      "metadata": {
        "id": "AfXayiiQhLnL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A first recurrent baseline"
      ]
    },
    {
      "metadata": {
        "id": "UOcNZ75Fhx-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using recurrent dropout to fight overfitting"
      ]
    },
    {
      "metadata": {
        "id": "Eyby7or-hyBs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Stacking recurrent layers"
      ]
    },
    {
      "metadata": {
        "id": "SKYdbp9ihyEa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Using bidirectional RNNs"
      ]
    },
    {
      "metadata": {
        "id": "6x7ZpxufhyHU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Going even further"
      ]
    },
    {
      "metadata": {
        "id": "IPVVlzE-hyKA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "P2iMcd00hyM7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6.4. Sequence processing with convnets"
      ]
    },
    {
      "metadata": {
        "id": "n9GkEtJMiEHg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Understanding 1D convolution for sequence data"
      ]
    },
    {
      "metadata": {
        "id": "u-CUQRVtiHni",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1D pooling for sequence data"
      ]
    },
    {
      "metadata": {
        "id": "jT1p2v30iJqy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Implementing a 1D convnet"
      ]
    },
    {
      "metadata": {
        "id": "ljC4CpZ0iMME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Combing CNNs and RNNs to process long sequences"
      ]
    },
    {
      "metadata": {
        "id": "QxYz4UVhiMPW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping up"
      ]
    },
    {
      "metadata": {
        "id": "O4P30oFPiMSW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Chapter summary"
      ]
    },
    {
      "metadata": {
        "id": "q0-YNz-FhyP_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}