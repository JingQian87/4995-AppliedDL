{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note_DLwPython_C5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JingQian87/AppliedDL/blob/master/Note_DLwPython_C5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ipgc5yi3kaYV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Chapter 5. Deep learning for computer vision"
      ]
    },
    {
      "metadata": {
        "id": "0-6QsOEVkas7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convnets**: convolutional neural network, in CV application."
      ]
    },
    {
      "metadata": {
        "id": "ZwgwzttJZ_YB",
        "colab_type": "code",
        "outputId": "c1af2347-8474-4b86-f314-fc9f12abfd16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AblQAzktk5Q-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "convnets take input insors (height, width, channels/depth).\n",
        "每个conv2D & MaxPooling2D layer输出是3D tesnsor of (height, width, channels).\n",
        "随着go deeper in the network, height & width 会shrink，而channels 数量是Conv2D的第一个参数（32/64）。\n",
        "\n",
        "接下来将结果（3，3，64）输出到densely connected classifier network: a stack of Dense Layers. 因为classifier输入是1维的，所以要把convnets的输出flatten."
      ]
    },
    {
      "metadata": {
        "id": "5EHEnIMAjRm4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wpSDQrZk2uZ",
        "colab_type": "code",
        "outputId": "285c52fc-c2d4-4cda-8ae3-c43d1f037d1d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                36928     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 93,322\n",
            "Trainable params: 93,322\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S9Loa7BHjd9z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "这里是10-way classifier。而在conv2d到dense的flatten是3*3*64 = 576."
      ]
    },
    {
      "metadata": {
        "id": "pR1tFjppjX5c",
        "colab_type": "code",
        "outputId": "0606c6a1-73d0-4f64-e889-0289c63a93cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "# Training the convnet on MNIST images\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 63s 1ms/step - loss: 0.1780 - acc: 0.9445\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0494 - acc: 0.9849\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0349 - acc: 0.9897\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0266 - acc: 0.9920\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.0216 - acc: 0.9936\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11e9fdfda0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "DglpD7uQjX2C",
        "colab_type": "code",
        "outputId": "6d5618eb-51c9-4fb5-cc0c-be029d7b7295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test data:\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "test_acc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 319us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9911"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "8rTJlND5k3Gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1.1. The convolution operation\n",
        "#### 1. Fundamental difference between dense and convulution layer: <br>\n",
        "dense learns **global** patterns,<br>\n",
        "convnets learns **local** patterns.  \n",
        "\n",
        "#### 2. Two properties of convnets:<br>\n",
        "2.1. **Patterns learnt are translation invariant**. efficient when processing images, need fewer training samples for generalization power.<br>\n",
        "2.2. **Can learn spatial hierarchies of patterns**. 第一层是小的局域pattern，比如edges; 第二层会是第一层feature组合成的大点的pattern等等。efficiently learn increasingly complex and abstract visual concepts.\n",
        "\n",
        "#### 3. Feature maps: \n",
        "3D tensors, 2 spatial axes (height+width) + depth/channels axis.\n",
        "对于RGB，# in depth = 3; 黑白，# in depth = 1.\n",
        "\n",
        "convnets输出的还是3D tensors => output feature map. 这里depth不再是输入的代表颜色的，#channels由参数确定，被称为filters, 理解为\"presence of a fact in the input\".\n",
        "\n",
        "#### 4. Two key parameters of convolutions. Conv2D(output_depth, (window_height, window_width))\n",
        "4.1. **size of the patches extracted from inputs**，通常3x3或5x5，就是Conv2D(32, (3, 3))里的(3,3)。\n",
        "\n",
        "4.2. **Depth of the output future map**, 这里我们开始是32，结束是64.\n",
        "\n",
        "#### 5. Concolution work流程(Fig5.4)\n",
        "在input feature map上滑动, 比如截取3 x 3 x input depth的patches; <br>\n",
        "patches与convolutional kernel点乘变成1D (1 x output depth)的transformed patches; <br>\n",
        "将上面的组装起来，变成3 x 3 x output depth 的output feature map.\n",
        "\n",
        "#### 6. Border effects and padding\n",
        "**Border effects**, 如果逐渐挪动，每个patches输出一列结果，得到的output与input相比，height-2且width-2。比如上面的例子中，28x28变成了26 x 26.\n",
        "\n",
        "如果希望输出仍然是28 x 28，就需要padding. 比如在上下各垫一行，左右各垫一列。\n",
        "\n",
        "tf.layers.conv2d(..., padding='valid',...)这里可以选'valid'或'same'。'valid'是no padding, 默认；'same'意思是输出与输入的height, width相同。\n",
        "\n",
        "\n",
        "#### 6. Convolution strides\n",
        "tf.layers.conv2d(..., strides=(1, 1),...)，单次滑动步长。默认是1.比如5x5的input 取3x3 convolution，一共是（5-2)x(5-2)=9个。\n",
        "实际运用中，downsample feature maps, 不用这个，而是max-pooling。\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "waMlAE_fQJk1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1.2. The max-pooling operation\n",
        "#### 1.Role: downsample feature maps.\n",
        "从input feature maps中提取windows, 然后输出每个channel最大值。\n",
        "与convolution不同，它通过hardcoded max tensor operation进行(而不是用convolution kernal transform)。\n",
        "常用的事2x2 window, stride 2,这样就可以downsample the feature maps by 2. \n",
        "从之前的model summary可以看到，size从26x26变成13x13。\n",
        "\n",
        "#### 2. 比逐步conv2d的好处：\n",
        "2.1. 有利于spatial hierarchy of features. <br>\n",
        "2.2. 不会像后者一样产生超级多的参数，造成overfitting.\n",
        "\n",
        "#### 3. 其它downsampling methods\n",
        "还有是取average pooling的，就是take average value of each channel over the patch, 而不是max.\n",
        "但是max pooling结果更好。\n",
        "\n",
        "最合理的方式是：<br>\n",
        "first produce dense maps of features (via unstrided convolutions) <br>\n",
        "look at the maximal activation of the features over small patches <br>\n",
        "也就是像上面的程序里一样，先conv2d产生很多小的patches, 然后对小patches进行maxpooling.\n",
        "\n",
        "<font color=red>可是为什么要conv2d, maxpooling间隔进行呢？为什么不能conv2d, conv2d, maxpooling, maxpooling? 有什么讲究么 <font>"
      ]
    },
    {
      "metadata": {
        "id": "tNhG765CUUdv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5.2. Training a convnet from scratch on a small dataset\n",
        "\n",
        "#### 1. 不一定要数据量很大才能做：<br>\n",
        "1. 可能模型本身就简单，数据又很regularized. convnets local的性质，data efficiency. <br>\n",
        "2. 可以用pretrained model based on large datasets.\n",
        "\n",
        "几乎所有的 convnets的pattern: 随着network的深入，depth逐渐变大，而size 逐渐变小。\n",
        "\n",
        "#### 2. Data preprocessing:\n",
        "1) Read picture files <br>\n",
        "2) Decode JPEG to RGB grids of pixels <br>\n",
        "3) Convert into floating-point tensors. <br>\n",
        "4) Rescale pixel values (0~255) to [0,1] (**neural networks prefer small input values**) \n",
        "\n",
        "程序中:<br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8Z1z3Fg4_k9S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)#Rescales all images by 1/255\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(150, 150),#Resizes all images to 150 × 150\n",
        "        batch_size=20,#number of samples in a batch\n",
        "        class_mode='binary')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2uNhwQQHRLx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 3. 常用避免overfitting的方法：\n",
        "dropout\n",
        "weight decay (L2 regularization).\n",
        "\n",
        "这里介绍了**data augmentation**: 通过对现有数据进行(random)变形，产生更多数据，从而避免overfitting. "
      ]
    },
    {
      "metadata": {
        "id": "aWZIF93UwfKN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#data augmentation \n",
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40, #value in degrees (0–180)\n",
        "      width_shift_range=0.2, #translate in fraction of total width or height\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2, #shear transformations\n",
        "      zoom_range=0.2, #randomly zoom inside pictures.\n",
        "      horizontal_flip=True, #randomly flipping half the images horizontally\n",
        "      fill_mode='nearest') #strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tRIkgNKWx3r6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "虽然在进行data augmentation之后，产生的数据不会重复，但是由于本身的data sample是小的，数据相关性很高，还是可能overfit.\n",
        "需要在densely connected classifier前加上dropout.\n",
        "\n",
        "**所以整体的结构就是:** <br>\n",
        "1) convolutional base (Convnets几遍): <br>\n",
        "     Cov2D() <br>\n",
        "     Maxpooling() <br>\n",
        "2) Flattening() <br>\n",
        "3) Dropout() <br>\n",
        "4) Dense classifier <br>\n",
        "     Dense('relu') <br>\n",
        "     Dense('sigmoid') <br>\n",
        "<font color='red'> softmax?<font>\n",
        "  \n",
        "  \n",
        "  "
      ]
    },
    {
      "metadata": {
        "id": "o66CXPskzU8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5.3. Use pretrained convnet\n",
        "\n",
        "可以在不同物体的image间迁移模型\n",
        "\n",
        "这里用的是VGG16。<br>\n",
        "\n",
        "\n",
        "两种使用方式：feature extraction & fine-tuning\n",
        "\n",
        "#### 1. Feature extraction\n",
        "老的convolutional base+新的classifier.\n",
        "认为convnets更general, classifier 更specific.\n",
        "dense与object位置无关。\n",
        "\n",
        "若图像与training的实在差太大，只用原模型前面几层，而不是所有convolutional base.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "I_bv3dG-zUcO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16 \n",
        "conv_base = VGG16(weights='imagenet', #specifies the weight checkpoint from which to initialize the model. \n",
        "                  include_top=False, # including (or not) the densely connected classifier on top of the network\n",
        "                  input_shape=(150, 150, 3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6hDQoh-aC8Qb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "两种方式：<br>\n",
        "将conv输出保存，另外再跟dense. 快，但因为中间断下来，不能做data augmentation <br>\n",
        "直接连着dense, 可以做data augmentation, 但慢的多\n",
        "\n",
        "<font color='red'>为什么不能augmentation之后保存或者dense之前先transform呢？<font>"
      ]
    }
  ]
}