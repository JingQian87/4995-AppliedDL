{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_cols.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "W3q34ekFNxi3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classify Structured Data\n"
      ]
    },
    {
      "metadata": {
        "id": "fgQeWSx3N-UT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is a draft of a tutorial that shows how to classify structured data (e.g. tabular data that you might find in a CSV). We will use [Keras](https://www.tensorflow.org/guide/keras) to define our model, and [feature columns](https://www.tensorflow.org/guide/feature_columns) to describe how each column from the CSV should be represented. In the process we will:\n",
        "* Load a CSV file using Pandas\n",
        "* Explore the format of the dataset\n",
        "* Build an input pipeline with tf.data\n",
        "* Demonstrate how to use sevearl different types of feature columns\n",
        "* Build and train a model with Keras\n",
        "* Evaluate our accuracy\n",
        "\n",
        "## Overview\n",
        "\n",
        "Using [census data](https://archive.ics.uci.edu/ml/datasets/Census+Income) which contains data a person's age, education, marital status, and occupation (the *features*), we will try to predict whether or not the person earns more than 50,000 dollars a year (the *label*). We will train a neural network that, given an individual's information, outputs a number between 0 and 1. This can be interpreted as the probability that the individual has an annual income of over 50,000 dollars.\n",
        "\n",
        "Key Point: As a developer, think about how this data is used and the potential benefits and harm a model's predictions can cause. A model built on a dataset like this one could reinforce societal biases and disparities. Is each feature relevant to the problem you want to solve or will it introduce bias? For more information, read about [ML fairness](https://developers.google.com/machine-learning/fairness-overview/)."
      ]
    },
    {
      "metadata": {
        "id": "fTC5sgrmC7nX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMpZ_4MPO8M2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import TensorFlow and other libraries"
      ]
    },
    {
      "metadata": {
        "id": "PzU4lTjxCtUp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow.feature_column as fc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "keras = tf.keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qGzQPRD8PAVU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Download the Census dataset\n",
        "\n",
        "We will use a version of this dataset that has been lightly preprocessed (for consistent formatting), to minimize the code in this notebook."
      ]
    },
    {
      "metadata": {
        "id": "DQx00wc8CvXb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "URL = 'https://storage.googleapis.com/applied-dl/uci_census_cleaned.csv'\n",
        "data = keras.utils.get_file('uci_census_cleaned.csv', URL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DA4AryxdPI8E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Use Pandas to load and preprocess the data\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) is a Python library with many helpful utilities for loading and working with structured data. We will use Pandas in this tutorial to load and preprocess the dataset before classifying it with TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "gheaCaV7DN-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv('~/.keras/datasets/uci_census_cleaned.csv')\n",
        "dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNhEsNkPWNa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The last column in the above output (income_bracket) is the label we will predict. Notice it is represented as a string. We will use Pandas to convert it to a number (0.0 or 1.0)."
      ]
    },
    {
      "metadata": {
        "id": "tDLtnfR_DOYc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataframe['income_bracket'] = dataframe['income_bracket'].map(lambda x: x == '>50K')\n",
        "dataframe['income_bracket'] = dataframe['income_bracket'].astype(float)\n",
        "dataframe.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zbWpuhiTPbiO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Split the dataset into train, validation, and test\n",
        "\n",
        "The dataset we downloaded was a single CSV file. We will split this into train, validation, and test sets."
      ]
    },
    {
      "metadata": {
        "id": "5yBndiqZDP16",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(dataframe, test_size=0.1)\n",
        "train, val = train_test_split(train, test_size=0.1)\n",
        "print(len(train), 'train examples')\n",
        "print(len(val), 'validation examples')\n",
        "print(len(test), 'test examples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jq9hBUnWPj9O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create an input pipeline using tf.data\n",
        "\n",
        "Next, we will wrap the dataframes with [tf.data](https://www.tensorflow.org/guide/datasets). These enable us to use feature columns as a bridge to map from the columns in the Pandas dataframe, to features used to train our Keras model."
      ]
    },
    {
      "metadata": {
        "id": "-BJwvfMNDSjw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('income_bracket')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  ds = ds.repeat().batch(batch_size)\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y-mIj2TEDUxb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# We will use a small batch size at first in order to demo\n",
        "# how this code works\n",
        "batch_size = 5\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-5TSe8FhQCOK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we have created our input pipeline, lets explore what it returns."
      ]
    },
    {
      "metadata": {
        "id": "SWWZsJxlDWIK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for feature_batch, label_batch in train_ds.take(1):\n",
        "  print('All features:', list(feature_batch.keys()))\n",
        "  print('A batch of ages:', feature_batch['age'])\n",
        "  print('A batch of labels:', label_batch )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hlxrax28QQot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can see that the dataset returns a dictionary of column names (from the dataframe) that map to column values. In a moment, we will use feature columns to represent these in different ways. First, let's retrieve a batch of data and keep it in memory. We will use this batch demostrate each type of feature column."
      ]
    },
    {
      "metadata": {
        "id": "jmfS3eLJDXcm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "example_batch = list(train_ds.take(1))[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPsWoSOYQ05C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Demonstrate each type of feature column\n",
        "TensorFlow has several different types of feature columns you can use. Next, we will create one of each, and demonstrate how it is used to represent a batch of data."
      ]
    },
    {
      "metadata": {
        "id": "Y3Eg_lVnD2uY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def demo(feature_column):\n",
        "  feature_layer = keras.layers.DenseFeatures(feature_column)\n",
        "  print(feature_layer(example_batch).numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CPcldiULRVk-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numeric columns\n",
        "A [numeric column](https://www.tensorflow.org/api_docs/python/tf/feature_column/numeric_column) is the simplest type of column. It's used to represent real valued features. When using this column, your model will receive the column value unchanged."
      ]
    },
    {
      "metadata": {
        "id": "kD-yldOyEEO_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "age = tf.feature_column.numeric_column(\"age\")\n",
        "demo(age)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9adh53KzR0TW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bucketized columns\n",
        "Often, you don't want to feed a number directly into the model, but instead split its value into different categories based on numerical ranges. To do so, create a [bucketized column](https://www.tensorflow.org/api_docs/python/tf/feature_column/bucketized_column). For example, consider raw data that represents a person's age. Instead of representing age as a numeric column, we could split the age into several buckets."
      ]
    },
    {
      "metadata": {
        "id": "Ne-RO0dCEK63",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "age_buckets = tf.feature_column.bucketized_column(age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "demo(age_buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tX1DPA0SSTKz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Categorical columns\n",
        "In the census dataset, education is represented as a string (e.g. bachelors). We cannot feed strings directly to a model. Instead, we must first map them to numeric or categorical values. Categorical vocabulary columns provide a way to represent strings as a one-hot vector. The vocabulary can be loaded from a list using [categorical_column_with_vocabulary_list](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_list), or from a file using [categorical_column_with_vocabulary_file](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file). "
      ]
    },
    {
      "metadata": {
        "id": "nAIY-hOAERfB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "      'education', [\n",
        "          'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
        "          'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
        "          '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
        "\n",
        "education_one_hot = tf.feature_column.indicator_column(education)\n",
        "demo(education_one_hot)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "skey9u6jTGih",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Embedding columns\n",
        "Now, suppose instead of having just a few possible strings, we have a million (or more). For a number of reasons, as the number of categories grow large, it becomes infeasible to train a neural network using one-hot encodings. We can use an embedding column to overcome this limitation. Instead of representing the data as a one-hot vector of many dimensions, an [embedding column](https://www.tensorflow.org/api_docs/python/tf/feature_column/embedding_column) represents that data as a lower-dimensional, dense vector in which each cell can contain any number, not just 0 or 1. The size of the embedding (8, in the example below) is a parameter that must be tuned."
      ]
    },
    {
      "metadata": {
        "id": "2nkwuHsgEUl2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "education_embedding = tf.feature_column.embedding_column(education, dimension=8)\n",
        "demo(education_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTa63qeKTs-u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Hashed feature columns\n",
        "\n",
        "Another way to represent a categorical column with a large number of values is to use a [categorical_column_with_hash_bucket](https://www.tensorflow.org/api_docs/python/tf/feature_column/categorical_column_with_hash_bucket). This feature columns enables you to specify the number of categories in advanced (instead of providing a vocabulary file, or list). This feature column calculates a hash value of the input, then selects one of the `hash_bucket_size` buckets to encode a string.\n",
        "\n",
        "An important downside of this technique is there may be collisions in which different strings are mapped to the same bucket. In practice, this can work well for some datasets regardless."
      ]
    },
    {
      "metadata": {
        "id": "QwpUGf6zEoSV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "      'occupation', hash_bucket_size=1000)\n",
        "demo(tf.feature_column.indicator_column(occupation))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_Dd3xHsUlpz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Crossed feature columns\n",
        "Combining features into a single feature, better known as [feature crosses](https://developers.google.com/machine-learning/glossary/#feature_cross), enables a model to learn separate weights for each combination of features. Here, we will create a new feature that is the cross of age and education. As a feature cross results in many new features, they are represented with a hash for efficiency."
      ]
    },
    {
      "metadata": {
        "id": "mFRDOKdxE50L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "crossed_feature = tf.feature_column.crossed_column([age_buckets, education], hash_bucket_size=1000)\n",
        "demo(tf.feature_column.indicator_column(crossed_feature))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cSRtb_1hVBvV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train a model\n",
        "We have seen how to use many types of feature coilumns. Now we will use them to train a model. We have chosen the features used below somewhat arbitrarily (they have not been tuned to build an accurate model). If your aim is to build an accurate model, try a dataset of your own, and think carefully about which features are the most meaningful to include."
      ]
    },
    {
      "metadata": {
        "id": "PIxZRjQuFCzK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "age = tf.feature_column.numeric_column('age')\n",
        "education_num = tf.feature_column.numeric_column('education_num')\n",
        "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
        "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
        "hours_per_week = tf.feature_column.numeric_column('hours_per_week')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BtSlK-IZFIk3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    'education', [\n",
        "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
        "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
        "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
        "\n",
        "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    'marital_status', [\n",
        "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
        "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
        "\n",
        "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    'relationship', [\n",
        "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
        "        'Other-relative'])\n",
        "\n",
        "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
        "    'workclass', [\n",
        "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
        "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eraT4jbsFMjk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
        "    'occupation', hash_bucket_size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yLaaDcxgFPNA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "age_buckets = tf.feature_column.bucketized_column(\n",
        "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
        "\n",
        "education_occuputation = tf.feature_column.crossed_column(['education', 'occupation'], \n",
        "                              hash_bucket_size=1000)\n",
        "\n",
        "age_education_occuptation = tf.feature_column.crossed_column([age_buckets, 'education', 'occupation'],\n",
        "                              hash_bucket_size=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iPz8Kj8yVVDA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create a feature layer\n",
        "Now that we have defined our feature columns, we will use a [DenseFeatures](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/DenseFeatures) layer to input them to our Keras model."
      ]
    },
    {
      "metadata": {
        "id": "XRo1V2v2FTwS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "all_columns = [\n",
        "    age,\n",
        "    education_num,\n",
        "    capital_gain,\n",
        "    capital_loss,\n",
        "    hours_per_week,\n",
        "    tf.feature_column.indicator_column(workclass),\n",
        "    tf.feature_column.indicator_column(education),\n",
        "    tf.feature_column.indicator_column(marital_status),\n",
        "    tf.feature_column.indicator_column(relationship),\n",
        "    tf.feature_column.embedding_column(education_occuputation, dimension=8),\n",
        "    tf.feature_column.embedding_column(age_education_occuptation, dimension=8),\n",
        "    tf.feature_column.embedding_column(occupation, dimension=8),\n",
        "]\n",
        "\n",
        "feature_layer = keras.layers.DenseFeatures(all_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_igkSsjVldu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Earlier, we used a small batch size to demonstrate how the feature columns worked. We will now create a new input pipeline."
      ]
    },
    {
      "metadata": {
        "id": "8ZqfMGFeFU_G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
        "val_ds = df_to_dataset(val, batch_size=batch_size)\n",
        "test_ds = df_to_dataset(test, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fU7mNL36VrYh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create, compile, and train the model"
      ]
    },
    {
      "metadata": {
        "id": "iw6YjSvVFfW7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  feature_layer,\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KUz3EGrVFha9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.binary_crossentropy,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Eb0O0RiUFimY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(train_ds, \n",
        "          steps_per_epoch=len(train)//batch_size,\n",
        "          validation_data=val_ds, \n",
        "          validation_steps=len(val)//batch_size,\n",
        "          epochs=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2QJqVWS8FlDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_ds, steps=len(test) // batch_size)\n",
        "print(\"Accuracy\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8p8_qlKUVvVr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Next steps\n",
        "\n",
        "The best way to learn more about classifying structured data is to try it yourself. We suggest finding another dataset to work with, and training a model to classifying it, using code similar to the above. To improve accuracy, think carefully about which features to include in your model, and how they should be represented."
      ]
    }
  ]
}