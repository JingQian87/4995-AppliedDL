{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "custom-loss.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "46IqQPIbHy5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Write a custom loss function\n",
        "\n",
        "This notebook is a quick example that shows how to:\n",
        "\n",
        "* Write your own implementation of softmax.\n",
        "* Write your own implementation of cross entropy loss.\n",
        "* Compare your method against a built-in one, in this case: [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits).\n",
        "\n",
        "The above part is only a few lines of code. We'll also train a simple MNIST model using our implementation (just to have an end-to-end example). \n",
        "\n",
        "Note: this notebook is **not** an explainer of how softmax or cross entropy works, just the mechanics of writing your own version. "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sEgjyAz6rM9o"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the nightly build"
      ]
    },
    {
      "metadata": {
        "id": "d90of0YY2AwS",
        "colab_type": "code",
        "outputId": "4087e0a8-377d-4e1c-b51b-b7ac112fd5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190203)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190203)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWS4rAhy2ZLA",
        "colab_type": "code",
        "outputId": "258391cc-3365-4468-df65-31e56548cf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow â‰¥ 2.0 required"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7Qx4Lni3mhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.nn import relu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LyAl_xd2WQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "# Types are needed later when calculating loss\n",
        "# using the ```sparse_softmax_cross_entropy_with_logits``` we chose to \n",
        "# compare against.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UnOpC7wP2iJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yskf-gkc3W00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128)\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    x = relu(x)\n",
        "    x = self.d2(x)\n",
        "    return x \n",
        "  \n",
        "model = MyModel()\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cuews7VZ9bl9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1) Comparison point\n",
        "\n",
        "I thought it'd be helpful to compare our implementation against a built-in method [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits), so we can unpack what it's doing. Note: for a more modern example of a built-in loss function, check out this [example](https://github.com/random-forests/applied-dl/blob/master/examples/2.2-hello-subclassing.ipynb).\n",
        "\n",
        "Let's unpack the name for starters.\n",
        "\n",
        "* ```sparse``` indicates that our labels are integer encoded (as opposed to one-hot). If your labels were in one-hot format, you would use ```softmax_cross_entropy_with_logits``` instead.\n",
        "\n",
        "* The next part of the name is ```softmax_cross_entropy_with_logits``` -- why are these grouped together? Softmax activation is commonly followed by cross entropy loss, these are group together for convenience. "
      ]
    },
    {
      "metadata": {
        "id": "OQ96tjTX433W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def built_in_loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "          logits=logits, labels=labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DtHE0OEKrNWx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2) Our implementation of softmax followed by cross entropy loss\n",
        "\n",
        "Next, we'll write our own version of the above code. It's just the four line below. To debug or understand what this block is doing, you can break it up into smaller pieces, and print out the shapes and/or data as you go. Note:  you can also convert tensors to NumPy with ```.numpy()```."
      ]
    },
    {
      "metadata": {
        "id": "5E3rkKEsrPXC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def our_loss(logits, labels, n_classes=10):  \n",
        "  # softmax part\n",
        "  sm = tf.math.exp(logits) / tf.reduce_sum(tf.math.exp(logits), axis=1, keepdims=True)\n",
        "  sm = tf.clip_by_value(sm, 1e-7, 1 - 1e-7)\n",
        "  # loss part\n",
        "  labels = tf.one_hot(labels, n_classes, dtype=tf.float32)\n",
        "  return tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(sm), axis=1))  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwBCSgRPWbGZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Choose which loss function to use in our training loop\n",
        "\n",
        "Ours, or the built-in one."
      ]
    },
    {
      "metadata": {
        "id": "A1pX0XLR5rte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_on_batch(model, images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Forward pass\n",
        "    logits = model(images)\n",
        "    loss_one = built_in_loss(logits, labels)\n",
        "    loss_two = our_loss(logits, labels)    \n",
        "    \n",
        "  # Backward pass\n",
        "  # I'll use our implementation to update the gradients.\n",
        "  grads = tape.gradient(loss_two, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  return loss_one, loss_two"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q2Ji3CiksOfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since we're writing lower-level code, here also is a NumPy-style way of calculating accuracy. For a more modern example using object-oriented metrics (which you should use in practice), see this [example](https://github.com/random-forests/applied-dl/blob/master/examples/2.2-hello-subclassing.ipynb)."
      ]
    },
    {
      "metadata": {
        "id": "hB6lmhlF5nOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Low-level code ahead. \n",
        "# See the above note for a better way of  calculating accuracy in practice.\n",
        "def calc_accuracy(logits, labels):\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  acc = tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size\n",
        "  return acc * 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3diKQ01_5v5E",
        "colab_type": "code",
        "outputId": "f98b898e-3818-4e47-cc59-8c80413b3950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1687
        }
      },
      "cell_type": "code",
      "source": [
        "# Loop over the dataset, grab batchs, and train our model\n",
        "# As we go, verify the loss returned by our implementation is\n",
        "# the same as the built-in methods.\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  print(\"Epoch\", epoch + 1, \"\\n\")\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss_one, loss_two = train_on_batch(model, images, labels)\n",
        "    \n",
        "    # You can use something like this as a quick sanity check\n",
        "    tf.debugging.assert_near(loss_one, loss_two, atol=0.001, rtol=0.001)\n",
        "    \n",
        "    step = optimizer.iterations.numpy() \n",
        "    if step % 100 == 0:\n",
        "      print(\"Step\", step)\n",
        "      print(\"Built-in loss: %.4f, Our loss: %.4f\" % (loss_one.numpy(), loss_two.numpy()))\n",
        "      print(\"\")\n",
        "      \n",
        "  print('Train accuracy %.2f' % calc_accuracy(model(x_train), y_train))\n",
        "  print('Test accuracy %.2f\\n' % calc_accuracy(model(x_test), y_test))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 \n",
            "\n",
            "Step 100\n",
            "Built-in loss: 0.4477, Our loss: 0.4477\n",
            "\n",
            "Step 200\n",
            "Built-in loss: 0.3544, Our loss: 0.3544\n",
            "\n",
            "Step 300\n",
            "Built-in loss: 0.3427, Our loss: 0.3427\n",
            "\n",
            "Step 400\n",
            "Built-in loss: 0.1488, Our loss: 0.1488\n",
            "\n",
            "Train accuracy 94.62\n",
            "Test accuracy 94.36\n",
            "\n",
            "Epoch 2 \n",
            "\n",
            "Step 500\n",
            "Built-in loss: 0.2276, Our loss: 0.2276\n",
            "\n",
            "Step 600\n",
            "Built-in loss: 0.2211, Our loss: 0.2211\n",
            "\n",
            "Step 700\n",
            "Built-in loss: 0.1373, Our loss: 0.1373\n",
            "\n",
            "Step 800\n",
            "Built-in loss: 0.1791, Our loss: 0.1791\n",
            "\n",
            "Step 900\n",
            "Built-in loss: 0.0953, Our loss: 0.0953\n",
            "\n",
            "Train accuracy 96.36\n",
            "Test accuracy 95.80\n",
            "\n",
            "Epoch 3 \n",
            "\n",
            "Step 1000\n",
            "Built-in loss: 0.1585, Our loss: 0.1585\n",
            "\n",
            "Step 1100\n",
            "Built-in loss: 0.1464, Our loss: 0.1464\n",
            "\n",
            "Step 1200\n",
            "Built-in loss: 0.1424, Our loss: 0.1424\n",
            "\n",
            "Step 1300\n",
            "Built-in loss: 0.0840, Our loss: 0.0840\n",
            "\n",
            "Step 1400\n",
            "Built-in loss: 0.1131, Our loss: 0.1131\n",
            "\n",
            "Train accuracy 97.21\n",
            "Test accuracy 96.40\n",
            "\n",
            "Epoch 4 \n",
            "\n",
            "Step 1500\n",
            "Built-in loss: 0.0510, Our loss: 0.0510\n",
            "\n",
            "Step 1600\n",
            "Built-in loss: 0.0980, Our loss: 0.0980\n",
            "\n",
            "Step 1700\n",
            "Built-in loss: 0.0594, Our loss: 0.0594\n",
            "\n",
            "Step 1800\n",
            "Built-in loss: 0.1690, Our loss: 0.1690\n",
            "\n",
            "Train accuracy 97.77\n",
            "Test accuracy 96.67\n",
            "\n",
            "Epoch 5 \n",
            "\n",
            "Step 1900\n",
            "Built-in loss: 0.0573, Our loss: 0.0573\n",
            "\n",
            "Step 2000\n",
            "Built-in loss: 0.0645, Our loss: 0.0645\n",
            "\n",
            "Step 2100\n",
            "Built-in loss: 0.0564, Our loss: 0.0564\n",
            "\n",
            "Step 2200\n",
            "Built-in loss: 0.0579, Our loss: 0.0579\n",
            "\n",
            "Step 2300\n",
            "Built-in loss: 0.0590, Our loss: 0.0590\n",
            "\n",
            "Train accuracy 98.19\n",
            "Test accuracy 97.00\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}