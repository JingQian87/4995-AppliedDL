{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3-1-backprop.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "_bV5iR44Y_CJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Calculating gradients in three ways"
      ]
    },
    {
      "metadata": {
        "id": "9k9tCirK536t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the example from Chris Olah's [Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/), we'll show how to calcuate gradients for the function `f(a, b) = (a + b) * (b + 1)` in three different ways:\n",
        "\n",
        "1. Numerically (nudge each parameter a little) -- simple, fun, slow.\n",
        "2. Analytically (backprop by hand, remembering our rules from calculus) -- efficient, tricky.\n",
        "3. Analytically (reverse mode autodiff in TensorFlow 2.0) -- efficient, automatic.\n",
        "\n",
        "After that, as a recommended exercise, I suggest watching this helpful [video](https://www.youtube.com/watch?v=d14TUNcbn1k) from Stanford's cs231 -- one of the clearest explanations of backprop on the web. Try to calculate the gradients for one of their examples using these methods, and verify you can get the same result with each. \n",
        "\n",
        "* I've included a solution for the first function: `f(x, y, z) = (x + y) * z`. \n",
        "\n",
        "At the end, there's a matrix example for a dense layer with ReLU activation.\n",
        "\n",
        "* f(W, x) = ReLU(Wx)"
      ]
    },
    {
      "metadata": {
        "id": "iZs4NzdTRr_s",
        "colab_type": "code",
        "outputId": "01f19d07-1a59-4001-f8bb-e4779de4cfdd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190206)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190206)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "42ehAJbDRy-L",
        "colab_type": "code",
        "outputId": "b6ee60b6-0c87-4ede-8fb9-80f6b6634e39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow â‰¥ 2.0 required"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190206\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "96R1t8fgK_EN"
      },
      "cell_type": "markdown",
      "source": [
        "## First example"
      ]
    },
    {
      "metadata": {
        "id": "BiSC_tWl-TFN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our first function will be `f(a, b) = (a + b) * (b + 1)`. "
      ]
    },
    {
      "metadata": {
        "id": "I80fotQCK2mW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Forward pass"
      ]
    },
    {
      "metadata": {
        "id": "Le6GqLcCIo1S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# forward pass, creating intermediate variables as we go\n",
        "def forward(a, b):\n",
        "  c = a + b\n",
        "  d = b + 1\n",
        "  f = c * d\n",
        "  return f"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AnFrL5YrXCmo",
        "colab_type": "code",
        "outputId": "9519f998-bb28-463e-d522-4e28166b8148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "forward(2,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "metadata": {
        "id": "yNK-xN7FLDI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numeric gradient"
      ]
    },
    {
      "metadata": {
        "id": "1kXRmBCvI-3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def numeric_gradient(my_function, params, h=1e-4):\n",
        "  \n",
        "  # Vector of partial derivatives\n",
        "  grads = []\n",
        "  \n",
        "  # Nudge each parameter by h\n",
        "  # calculate how much the function changes\n",
        "  for i in range(len(params)):\n",
        "    \n",
        "    orginal_val = params[i]\n",
        "    \n",
        "    # f(x + h)\n",
        "    params[i] = orginal_val + h\n",
        "    plus_h = my_function(*params) \n",
        "        \n",
        "    # f(x - h)\n",
        "    params[i] = orginal_val - h\n",
        "    minus_h = my_function(*params)\n",
        "        \n",
        "    # Partial derivative\n",
        "    grad = (plus_h - minus_h) / (2 * h)\n",
        "    grads.append(grad)\n",
        "    \n",
        "    # reset\n",
        "    params[i] = orginal_val\n",
        "       \n",
        "  return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6PWzE2UWI4if",
        "colab_type": "code",
        "outputId": "3c3e5e57-b21b-4ff6-e9b4-8ffe1c26a546",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "da, db = numeric_gradient(my_function=forward, params=[2, 1])\n",
        "print (\"Numeric gradient. da %0.2f, db %0.2f\" % (da, db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numeric gradient. da 2.00, db 5.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-MEhR5pfLLkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (by hand)\n",
        "Backprop by hand, if we rememeber our calculus rules."
      ]
    },
    {
      "metadata": {
        "id": "SVPs3gwCLOw5",
        "colab_type": "code",
        "outputId": "7b369281-9074-4a23-aff8-a3771c366b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "def backprop_by_hand(a, b):\n",
        "  \n",
        "  # forward pass, keeping track of intermediate variables\n",
        "  c = a + b\n",
        "  d = b + 1\n",
        "  f = c * d\n",
        "  \n",
        "  df = 1 # Gradient of f itself is 1 (this would be replaced by loss in a model)\n",
        "  dc = d # Product rule\n",
        "  dd = c # Product rule\n",
        "\n",
        "  dc_da = 1 # Sum rule\n",
        "  da = dc * dc_da\n",
        "  \n",
        "  dc_db = 1 # Sum rule\n",
        "  db_path_1 = dc * dc_db\n",
        "  \n",
        "  dd_db = 1 # Sum rule\n",
        "  db_path_2 = dd * dd_db\n",
        "  \n",
        "  db = db_path_1 + db_path_2 # Sum over paths (multivariate chain rule)\n",
        "  \n",
        "  return da, db\n",
        "\n",
        "da, db = backprop_by_hand(a=2, b=1)\n",
        "print (\"Gradient by backprop. da %0.2f, db %0.2f\" % (da, db))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient by backprop. da 2.00, db 5.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CVBlIAc8LzJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (autodiff)\n",
        "Reverse mode autodiff using TensorFlow.\n",
        "\n",
        "Using a GradientTape ([doc](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape), [short tutorial](https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/eager/automatic_differentiation.ipynb))."
      ]
    },
    {
      "metadata": {
        "id": "vj18HaqQJAbk",
        "colab_type": "code",
        "outputId": "618d13da-f849-4dd2-8498-7622ff22b1e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "params = [tf.Variable(2.0), tf.Variable(1.0)]\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, params)\n",
        "\n",
        "print(\"Analytic gradient (by autodiff). da %0.2f, db %0.2f\" % \n",
        "      (grads[0].numpy(), grads[1].numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytic gradient (by autodiff). da 2.00, db 5.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9qMiGDvPQ-nk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Second example\n",
        "\n",
        "Here's `f(x, y, z) = (x + y) * z` in the same three ways."
      ]
    },
    {
      "metadata": {
        "id": "vAfpNI12RM8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward(x, y, z):\n",
        "  return (x + y) * z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-jaobiCEYX8n",
        "colab_type": "code",
        "outputId": "64f63071-7b4a-4720-f315-49aa865e2caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "forward(x=1, y=2, z=-4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "FnuTOZ8gR4bo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numeric gradient\n",
        "Same method as above."
      ]
    },
    {
      "metadata": {
        "id": "_LX3YK8mR8fr",
        "colab_type": "code",
        "outputId": "60a8f5df-2a2a-44c2-af21-c8ac06b7bb20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dx, dy, dz = numeric_gradient(my_function=forward, params=[1, 2, -4])\n",
        "print (\"Numeric gradient. dx %0.2f, dy %0.2f, dz %0.2f\" % (dx, dy, dz))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numeric gradient. dx -4.00, dy -4.00, dz 3.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z3zb-YFTRvQ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (by hand)\n",
        "Backprop by hand, if we rememeber our calculus rules."
      ]
    },
    {
      "metadata": {
        "id": "4P5HWOdx5yfO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def backprop_by_hand(x, y, z):\n",
        "  \n",
        "  # Calculate the forward pass, creating intermediate variables as we go.\n",
        "  q = x + y\n",
        "  f = q * z\n",
        "  \n",
        "  # Propagate the gradient on the function's output \n",
        "  # backward through intermediate variables, until we reach the inputs.\n",
        "  \n",
        "  # Gradient on the function is just itself. In a model, this would \n",
        "  # be replaced by the loss.\n",
        "  df = 1 \n",
        "  \n",
        "  # By product rule.\n",
        "  dq = z\n",
        "  \n",
        "  # By product rule.\n",
        "  dz = q\n",
        "  \n",
        "  dq_dx = 1 # By sum rule\n",
        "  dq_dy = 1 # By sum rule\n",
        "  \n",
        "  # Backprop gradient on q into x\n",
        "  dx = dq * dq_dx\n",
        "  \n",
        "  # Backprop gradient on q into y\n",
        "  dy = dq * dq_dy\n",
        "    \n",
        "  return dx, dy, dz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F6c8yzX8Rmbn",
        "colab_type": "code",
        "outputId": "7152550b-34d5-4f91-967c-3206f6e1a575",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "dx, dy, dz = backprop_by_hand(1., 2., -4.)\n",
        "print (\"Gradient by backprop. dx %0.2f, dy %0.2f, dz %0.2f\" % (dx, dy, dz))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient by backprop. dx -4.00, dy -4.00, dz 3.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4uNh7itnSYLj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (autodiff)\n",
        "Reverse mode autodiff using TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "t0gsYnKEScTs",
        "colab_type": "code",
        "outputId": "bac0100e-c065-461a-d337-af0b6cb5b901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "params = [tf.Variable(1.0), tf.Variable(2.0), tf.Variable(-4.0)]\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, params)\n",
        "\n",
        "print(\"Analytic gradient (by autodiff). dx %0.2f, dy %0.2f dz %0.2f\" % \n",
        "      (grads[0].numpy(), grads[1].numpy(), grads[2].numpy()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytic gradient (by autodiff). dx -4.00, dy -4.00 dz 3.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sadXGr2_SR9N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Third example\n",
        "\n",
        "`f(W, x) = ReLU(Wx)`"
      ]
    },
    {
      "metadata": {
        "id": "XL2PFlo5XW8Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def forward(W, x):\n",
        "    wx = tf.matmul(W,x)\n",
        "    result = tf.nn.relu(wx)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jg9c25M-XYhr",
        "colab_type": "code",
        "outputId": "8475d7d8-61c4-4233-fc7b-65f02281412e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "W = np.array([[-0.5, 0.2],\n",
        "              [-0.3, 0.8]])                  \n",
        "x = np.array([0.2, 0.4])  \n",
        "x = np.expand_dims(x,1)   \n",
        "forward(W,x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=194, shape=(2, 1), dtype=float64, numpy=\n",
              "array([[0.  ],\n",
              "       [0.26]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "srouRWQCZEm5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Numeric gradient"
      ]
    },
    {
      "metadata": {
        "id": "W2geWmTqokYb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Updated code that supports matmul\n",
        "def numeric_gradient_v2(my_function, weights, inputs, h=1e-4):\n",
        "  \n",
        "  # Vectors of partial derivatives\n",
        "  grads = np.zeros_like(weights)\n",
        "  \n",
        "  for i in range(grads.shape[0]):\n",
        "    for j in range(grads.shape[1]):\n",
        "      \n",
        "      original = weights[i,j]\n",
        "      \n",
        "      weights[i,j] = original + h\n",
        "      plus_h = my_function(weights, inputs)\n",
        "      \n",
        "      weights[i,j] = original - h\n",
        "      minus_h = my_function(weights, inputs)\n",
        "      \n",
        "      # partial\n",
        "      grads[i,j] = np.sum(plus_h - minus_h) / (2 * h)\n",
        "      \n",
        "      # reset\n",
        "      weights[i,j] = original\n",
        "      \n",
        "  return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x88WSpuk0Nci",
        "colab_type": "code",
        "outputId": "6a4be3dc-efa6-46bc-d4cf-dafdb22146cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "dw = numeric_gradient_v2(forward, weights=W, inputs=x)\n",
        "print (\"Numeric gradient. dw\\n\" % dw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numeric gradient. dw\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bcxytV9BZLd7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (by hand)\n",
        "Backprop by hand, if we rememeber our calculus rules."
      ]
    },
    {
      "metadata": {
        "id": "vWyyOJef0Sol",
        "colab_type": "code",
        "outputId": "cbe58a54-ee00-4e21-c80a-01addb936b1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "# Backprop\n",
        "def backward(W, x):\n",
        "  q = np.maximum(0, np.matmul(W, x))\n",
        "  dq = np.ones_like(q)\n",
        "  dq[q <= 0] = 0  \n",
        "  dw = np.matmul(dq, x.T)\n",
        "  return dw\n",
        "\n",
        "dw = backward(W,x)\n",
        "print (\"Gradient by hand\\n\", dw)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient by hand\n",
            " [[0.  0. ]\n",
            " [0.2 0.4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PBKP_eS7ZMf5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analytic gradient (autodiff)\n",
        "Reverse mode autodiff using TensorFlow."
      ]
    },
    {
      "metadata": {
        "id": "6-m03wlhztHT",
        "colab_type": "code",
        "outputId": "47dc6164-d2b6-49c6-d3e4-fecfcd0f41d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "W = tf.Variable([[-0.5, 0.2],\n",
        "                 [-0.3, 0.8]])                  \n",
        "x = tf.Variable([0.2, 0.4])  \n",
        "x = tf.expand_dims(x,1)   \n",
        "\n",
        "params = [W, x]\n",
        "with tf.GradientTape() as tape:\n",
        "  result = forward(*params)\n",
        "grads = tape.gradient(result, W)\n",
        "\n",
        "print(\"Analytic gradient (by autodiff). dw\", grads)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytic gradient (by autodiff). dw tf.Tensor(\n",
            "[[0.  0. ]\n",
            " [0.2 0.4]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}