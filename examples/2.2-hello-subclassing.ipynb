{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2b: hello-subclassing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "46IqQPIbHy5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hello World: Subclassing and GradientTape edition\n",
        "\n",
        "An example showing how to use Keras [Subclassing](https://www.tensorflow.org/guide/keras) in TensorFlow 2.0. We'll use a [GradientTape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape) to write our training loop. You can find more details about this style, and how it compares to the previous one, in this [article](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021)."
      ]
    },
    {
      "metadata": {
        "id": "Bp8cvZHw-Lee",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Install the nightly build\n"
      ]
    },
    {
      "metadata": {
        "id": "d90of0YY2AwS",
        "colab_type": "code",
        "outputId": "a25240a4-52f4-4822-ff93-9a0fbe7f3967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190203)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a0,>=1.13.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0a20190203)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.13.0.dev2019012800)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.1.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TWS4rAhy2ZLA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KN9zsDk-2H3-",
        "colab_type": "code",
        "outputId": "785c5493-8d32-47a2-8ceb-0465d6eb8b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"You have version\", tf.__version__)\n",
        "assert tf.__version__ >= \"2.0\" # TensorFlow â‰¥ 2.0 required"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "You have version 2.0.0-dev20190203\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7Qx4Lni3mhh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GssK6Svd7QtE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "metadata": {
        "id": "1LyAl_xd2WQp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QvShJo97L2i9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Batch and shuffle the data"
      ]
    },
    {
      "metadata": {
        "id": "abmLlAjjKe9r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we'll use `tf.data` to batch up and shuffle our dataset. Notice the `buffer_size` parameter. Why is it necessary? Datasets are (potentially infinite) streams. Since we can't shuffle a stream, we maintain a buffer, and shuffle that instead."
      ]
    },
    {
      "metadata": {
        "id": "UnOpC7wP2iJG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "mnist_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "mnist_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N_uGXeW_7UOx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define a model\n",
        "\n",
        "Using this style feels like Object-Oriented Python + NumPy development. Initialize your layers in the constructor, then write your forward pass in the call method."
      ]
    },
    {
      "metadata": {
        "id": "Yskf-gkc3W00",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MyModel(Model):\n",
        "  def __init__(self):\n",
        "    super(MyModel, self).__init__()\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(10, activation='softmax')\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "  \n",
        "model = MyModel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XX1_AHnKjRNU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Choose an optimizer and loss function"
      ]
    },
    {
      "metadata": {
        "id": "buwNpDwvjWW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TGU7O1Cki0i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Choose metrics to measure loss and accuracy\n",
        "These are helper functions that accumulate values over time."
      ]
    },
    {
      "metadata": {
        "id": "GLdAlTkokihO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_loss_metric = tf.keras.metrics.Mean(name='train_loss')\n",
        "test_loss_metric = tf.keras.metrics.Mean(name='test_loss')\n",
        "\n",
        "train_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "test_accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3Im20vF7-G0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Train the model using GradientTape\n",
        "\n",
        "We'll use a [GradientTape](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/GradientTape), rather than the built-in `model.fit`.\n",
        "\n",
        "Also note, we could \"compile\" the next two methods by adding a [tf.function](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/function)  annotation at the top. Not necessary for our purposes here, just FYI. "
      ]
    },
    {
      "metadata": {
        "id": "A1pX0XLR5rte",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:    \n",
        "    # Forward pass\n",
        "    predictions = model(images)\n",
        "    train_loss = loss_function(y_true=labels, y_pred=predictions)\n",
        "  \n",
        "  # Backward pass\n",
        "  gradients = tape.gradient(train_loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  # Record results\n",
        "  train_loss_metric(train_loss)\n",
        "  train_accuracy_metric(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dTcVGhUqlgpK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_step(images, labels):\n",
        "  predictions = model(images)\n",
        "  test_loss = loss_function(y_true=labels, y_pred=predictions)\n",
        "  \n",
        "  # Record results\n",
        "  test_loss_metric(test_loss)\n",
        "  test_accuracy_metric(labels, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3diKQ01_5v5E",
        "colab_type": "code",
        "outputId": "195df06c-a9e8-484c-baad-93486aa2a640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for images, labels in mnist_train:\n",
        "    train_step(images, labels)\n",
        "  \n",
        "  for test_images, test_labels in mnist_test:\n",
        "    test_step(test_images, test_labels)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {:.4f}, Accuracy: {:.2f}, Test loss: {:.4f}, Test accuracy: {:.2f}'\n",
        "  print (template.format(epoch +1, \n",
        "                         train_loss_metric.result(), \n",
        "                         train_accuracy_metric.result() * 100, \n",
        "                         test_loss_metric.result(), \n",
        "                         test_accuracy_metric.result() * 100))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.3517, Accuracy: 90.32, Test loss: 0.1958, Test accuracy: 94.26\n",
            "Epoch 2, Loss: 0.2572, Accuracy: 92.80, Test loss: 0.1678, Test accuracy: 94.99\n",
            "Epoch 3, Loss: 0.2104, Accuracy: 94.08, Test loss: 0.1499, Test accuracy: 95.52\n",
            "Epoch 4, Loss: 0.1804, Accuracy: 94.92, Test loss: 0.1373, Test accuracy: 95.89\n",
            "Epoch 5, Loss: 0.1588, Accuracy: 95.52, Test loss: 0.1279, Test accuracy: 96.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FFRdwFfm3z2q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Okay! As a next step, you can play with the model to see if you can increase the accuracy."
      ]
    }
  ]
}