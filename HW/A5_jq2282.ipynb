{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A5_jq2282.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Nl9O4i7mv4f0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 5 Machine Translation\n",
        "### Jing Qian (jq2282)"
      ]
    },
    {
      "metadata": {
        "id": "7z55z28i738n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1. Install packages and load libraries\n"
      ]
    },
    {
      "metadata": {
        "id": "Xq06vFFe7-n-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bVtBFdMz8CAL",
        "colab_type": "code",
        "outputId": "433812e4-e27e-431b-d11f-7fc840a82a1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu # https://github.com/mjpost/sacreBLEU"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from sacrebleu) (3.6.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2iTUshdZ8GVd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sacrebleu\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unicodedata"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hsLPnQ7awRjf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2. Load dataset for English to Spanish"
      ]
    },
    {
      "metadata": {
        "id": "0wJTsIVehUJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ntrain = 500\n",
        "ntest = 100\n",
        "nall = ntrain + ntest\n",
        "nepoch = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UD7o60AKwTmW",
        "colab_type": "code",
        "outputId": "7bb6a788-9e05-4f9f-9d24-1465b8d31350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uFVeZLQAww7K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load(fname):\n",
        "  # Load the file using std open\n",
        "  f = open(fname, 'r')\n",
        "  text = []\n",
        "  for line in f.readlines():\n",
        "    text.append(line.replace('\\n','').split('\\t'))\n",
        "    \n",
        "  f.close()\n",
        "  return text\n",
        "\n",
        "data = load('/content/gdrive/My Drive/spa-eng/spa.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OL1LWQaQzh-0",
        "colab_type": "code",
        "outputId": "7b4932b6-18f4-4e36-a6d9-746129ea9afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "print(data[10:15])\n",
        "print(np.shape(data))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Fire!', '¡Disparad!'], ['Help!', '¡Ayuda!'], ['Help!', '¡Socorro! ¡Auxilio!'], ['Help!', '¡Auxilio!'], ['Jump!', '¡Salta!']]\n",
            "(118964, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ok7f6Amz7nC0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start without worrying about duplicate translations\n",
        "* Shuffle the dataset\n",
        "* Randomly choose 5000 sentences as training, 1000 as validation."
      ]
    },
    {
      "metadata": {
        "id": "4qcTrL_E8iws",
        "colab_type": "code",
        "outputId": "4580f436-ced8-452a-b93a-bcfc55682582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "shuffled_data = np.random.permutation(data)\n",
        "selected_id = np.random.randint(len(data), size = nall)\n",
        "train_data = shuffled_data[selected_id[:ntrain], :]\n",
        "val_data = shuffled_data[selected_id[ntrain:], :]\n",
        "print(np.shape(train_data), np.shape(val_data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 2) (100, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z486M-O-wBk9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3. Train a model to translate from English to Spanish\n"
      ]
    },
    {
      "metadata": {
        "id": "oA-oSGBJSmnr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(sentences, EPOCHS):\n",
        "  def preprocess(s):\n",
        "    # for details, see https://www.tensorflow.org/alpha/tutorials/sequences/nmt_with_attention\n",
        "    s = ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "    s = s.strip()\n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s\n",
        "\n",
        "  print(\"Original:\", sentences[0])\n",
        "  sentences = [(preprocess(source), preprocess(target)) for (source, target) in sentences]\n",
        "  print(\"Preprocessed:\", sentences[0])\n",
        "\n",
        "  source_sentences, target_sentences = list(zip(*sentences))\n",
        "  \n",
        "  source_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  source_tokenizer.fit_on_texts(source_sentences)\n",
        "  source_data = source_tokenizer.texts_to_sequences(source_sentences)\n",
        "  print(\"Sequence:\", source_data[0])\n",
        "  source_data = tf.keras.preprocessing.sequence.pad_sequences(source_data, padding='post')\n",
        "  print(\"Padded:\", source_data[0])\n",
        "  \n",
        "  target_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "  target_tokenizer.fit_on_texts(target_sentences)\n",
        "  target_data = target_tokenizer.texts_to_sequences(target_sentences)\n",
        "  target_data = tf.keras.preprocessing.sequence.pad_sequences(target_data, padding='post')\n",
        "\n",
        "  # Create labels for the decoder by shifting the target sequence\n",
        "  # one to the right.\n",
        "  target_labels = np.zeros(target_data.shape)\n",
        "  target_labels[:,0:target_data.shape[1] -1] = target_data[:,1:]\n",
        "\n",
        "  print(\"Target sequence\", target_data[0])\n",
        "  print(\"Target label\", target_labels[0])\n",
        "  \n",
        "  source_vocab_size = len(source_tokenizer.word_index) + 1\n",
        "  target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "  \n",
        "  def decode(encoded, tokenizer):\n",
        "    for number in encoded:\n",
        "      if number !=0:\n",
        "        print (\"%d -> %s\" % (number, tokenizer.index_word[number]))\n",
        "\n",
        "  decode(source_data[0], source_tokenizer)\n",
        "  \n",
        "  batch_size = 5\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((source_data, target_data, target_labels)).batch(batch_size)\n",
        "  \n",
        "  example_batch = next(iter(dataset))\n",
        "  source, target, taget_labels = example_batch\n",
        "  print(\"Shapes:\", source.shape, target.shape, taget_labels.shape)\n",
        "  \n",
        "  embedding_size = 32\n",
        "  rnn_size = 64\n",
        "  \n",
        "  class Encoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "      super(Encoder, self).__init__()\n",
        "\n",
        "      self.embedding = tf.keras.layers.Embedding(source_vocab_size,\n",
        "                                                 embedding_size)\n",
        "      self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True)\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "      x = self.embedding(x)\n",
        "      output, state = self.gru(x, initial_state=hidden)        \n",
        "      return output, state\n",
        "\n",
        "    def init_state(self, batch_size):\n",
        "      return tf.zeros((batch_size, rnn_size))\n",
        "\n",
        "  # Create a batch of one sentence\n",
        "  ex_sentence = tf.expand_dims(source_data[0], axis=0)\n",
        "  ex_translation = tf.expand_dims(target_data[0], axis=0)\n",
        "  ex_labels = tf.expand_dims(target_labels[0], axis=0)\n",
        "  print(ex_sentence.shape)\n",
        "\n",
        "  encoder = Encoder()\n",
        "  hidden_state = encoder.init_state(batch_size=1)\n",
        "  print(hidden_state.shape)\n",
        "\n",
        "  output, hidden_state = encoder(ex_sentence, hidden_state)\n",
        "  print(output.shape)\n",
        "    \n",
        "  class Decoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.embedding = tf.keras.layers.Embedding(target_vocab_size, \n",
        "                                                 embedding_size)\n",
        "      self.gru = tf.keras.layers.GRU(rnn_size, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True)\n",
        "\n",
        "      self.dense = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "\n",
        "    def call(self, x, hidden):\n",
        "      x = self.embedding(x)\n",
        "      output, state = self.gru(x, initial_state=hidden)\n",
        "      logits = self.dense(output)\n",
        "      return logits, state   \n",
        "    \n",
        "  decoder = Decoder()\n",
        "  decoder_output, decoder_state = decoder(ex_labels, hidden_state)\n",
        "  print(decoder_output.shape)\n",
        "\n",
        "  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "  def calc_loss(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    return crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "  print(\"Loss\", calc_loss(ex_labels, decoder_output))  \n",
        "  \n",
        "  def translate(idx=None):\n",
        "  \n",
        "    if idx == None: \n",
        "      idx = np.random.choice(len(sentences))\n",
        "    \n",
        "    input_sent = source_data[idx]\n",
        "    input_sent = tf.expand_dims(input_sent, axis=0)\n",
        "    \n",
        "    hidden_state = encoder.init_state(batch_size=1)\n",
        "    output, hidden_state = encoder(input_sent, hidden_state)\n",
        "    \n",
        "    decoder_input = tf.expand_dims([target_tokenizer.word_index['<start>']], 0)\n",
        "    out_words = []\n",
        "    \n",
        "    decoder_state = hidden_state\n",
        "\n",
        "    while True:\n",
        "      \n",
        "        decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
        "        decoder_input = tf.argmax(decoder_output, -1)\n",
        "        word_idx = decoder_input.numpy()[0][0]\n",
        "        # if we've predicted 0 (which is reserved, usually this will only happen\n",
        "        # before the decoder is trained, just stop translating and return\n",
        "        # what we have)\n",
        "        if word_idx == 0: \n",
        "          out_words.append('<end>')\n",
        "        else:\n",
        "          out_words.append(target_tokenizer.index_word[word_idx])\n",
        "\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 20:\n",
        "          break\n",
        "          \n",
        "    translation = ' '.join(out_words)    \n",
        "    return sentences[idx][0], sentences[idx][1], translation\n",
        "  \n",
        "  input_sent, target_sent, translation = translate()\n",
        "  print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "  \n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  @tf.function # remove this annotation when debugging\n",
        "  def train_step(source_seq, target_seq, target_labels, initial_state):\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_output, encoder_state = encoder(source_seq, initial_state)\n",
        "      logits, decoder_state = decoder(target_seq, encoder_state)\n",
        "      loss = calc_loss(target_labels, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "  translated = []\n",
        "  for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "\n",
        "      en_initial_states = encoder.init_state(batch_size)\n",
        "\n",
        "      for batch, (source_seq, target_seq, target_labels) in enumerate(dataset):\n",
        "        loss = train_step(source_seq, target_seq, target_labels, en_initial_states)\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "      if epoch % 10 == 0:\n",
        "        print(\"Epoch #%d, Loss %.4f, Time %.2f sec\" % (epoch, loss, elapsed))\n",
        "        input_sent, target_sent, translation = translate()\n",
        "        translated.append((input_sent, target_sent, translation))\n",
        "        print(\"Input: %s\\nTarget: %s\\nTranslation: %s\\n\" % (input_sent, target_sent, translation))\n",
        "        \n",
        "  return translated       \n",
        "        \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBaUKYmaSmha",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "outputId": "0682d387-4d2d-4e58-a751-ce8ecb0ae613"
      },
      "cell_type": "code",
      "source": [
        "a = train_model(train_data, 30)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original: ['How do you think that makes me feel?'\n",
            " '¿Cómo crees que me hace sentir eso?']\n",
            "Preprocessed: ('<start> How do you think that makes me feel ? <end>', '<start> ¿ Como crees que me hace sentir eso ? <end>')\n",
            "Sequence: [1, 47, 20, 7, 88, 16, 369, 17, 103, 10, 2]\n",
            "Padded: [  1  47  20   7  88  16 369  17 103  10   2   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0]\n",
            "Target sequence [  1  10  35 127   4  15  47 193  36  11   2   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0]\n",
            "Target label [ 10.  35. 127.   4.  15.  47. 193.  36.  11.   2.   0.   0.   0.   0.\n",
            "   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "   0.]\n",
            "1 -> <start>\n",
            "47 -> how\n",
            "20 -> do\n",
            "7 -> you\n",
            "88 -> think\n",
            "16 -> that\n",
            "369 -> makes\n",
            "17 -> me\n",
            "103 -> feel\n",
            "10 -> ?\n",
            "2 -> <end>\n",
            "Shapes: (5, 24) (5, 29) (5, 29)\n",
            "(1, 24)\n",
            "(1, 64)\n",
            "(1, 24, 64)\n",
            "(1, 29, 1179)\n",
            "Loss tf.Tensor(2.4387212, shape=(), dtype=float32)\n",
            "Input: <start> He knows the truth . <end>\n",
            "Target: <start> El conoce la verdad . <end>\n",
            "Translation: limpiara corto tengo costara tino hablado circulan circulan circulan tino verdad atencion hubiera sentado viene suficiente cien ocho campo pulsa\n",
            "\n",
            "Epoch #0, Loss 1.4424, Time 7.43 sec\n",
            "Input: <start> Pick me up at . <end>\n",
            "Target: <start> Recogeme a las dos y media . <end>\n",
            "Translation: ¿ ¿ <end>\n",
            "\n",
            "Epoch #10, Loss 0.9590, Time 3.17 sec\n",
            "Input: <start> They got married . <end>\n",
            "Target: <start> Ellos se casaron . <end>\n",
            "Translation: ¿ es que se gusta , el se gusta , el dinero . <end>\n",
            "\n",
            "Epoch #20, Loss 0.7707, Time 3.11 sec\n",
            "Input: <start> They took no part in the social revolution . <end>\n",
            "Target: <start> Ellos no tomaron parte en la revolucion social . <end>\n",
            "Translation: ¿ es que se gusta , por que lo que lo lo lo por que lo que lo que lo\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ewq8aghhxlG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4. Train a model to translate from Spanish to English"
      ]
    },
    {
      "metadata": {
        "id": "P5cO43dph5_w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "haPb37i4ipin"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5. Back-translate\n",
        "Back-translate. Use your two models to translate a sentence from English to Spanish, and then back to English. Compare the original sentence, and the back-translated sentence. Repeat this using an evaluation corpus of 1,000 sentences, and report the BLEU score."
      ]
    },
    {
      "metadata": {
        "id": "2pu8_UF0X8eq",
        "colab_type": "code",
        "outputId": "dfd443e0-4c9e-4e7e-897d-99b1ae8bac55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        }
      },
      "cell_type": "code",
      "source": [
        "# Calculate BLEU score\n",
        "references, hypotheses = [], []\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "  input_sent, target_sent, translation = translate()\n",
        "  references.append(target_sent)\n",
        "  hypotheses.append(\"<start> \" + translation)\n",
        "  \n",
        "results = sacrebleu.raw_corpus_bleu(hypotheses, [references])\n",
        "print(results)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-5772ef563968>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0minput_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mreferences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CljUXQW2X8Yr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NeCt8OneX8bU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rCRTJbHbX8WE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0clfoTfeXYOL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M3-mKid9XFBb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1U3OBwyXE9f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vxGHqUtW75xF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "0_IZ0O7WSmkb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "  (\"Do you want a cup of coffee?\", \"¿Quieres una taza de café?\"),\n",
        "  (\"I've had coffee already.\", \"Ya tomé café.\"),\n",
        "  (\"Can I get you a coffee?\", \"¿Quieres que te traiga un café?\"),\n",
        "  (\"Please give me some coffee.\", \"Dame algo de café por favor.\"),\n",
        "  (\"Would you like me to make coffee?\", \"¿Quieres que prepare café?\"),\n",
        "  (\"Two coffees, please.\", \"Dos cafés, por favor.\"),\n",
        "  (\"How about a cup of coffee?\", \"¿Qué tal una taza de café?\"),\n",
        "  (\"I drank two cups of coffee.\", \"Me tomé dos tazas de café.\"),\n",
        "  (\"Would you like to have a cup of coffee?\", \"¿Te gustaría tomar una taza de café?\"),\n",
        "  (\"There'll be coffee and cake at five.\", \"A las cinco habrá café y un pastel.\"),\n",
        "  (\"Another coffee, please.\", \"Otro café, por favor.\"),\n",
        "  (\"I made coffee.\", \"Hice café.\"),\n",
        "  (\"I would like to have a cup of coffee.\", \"Quiero beber una taza de café.\"),\n",
        "  (\"Do you want me to make coffee?\", \"¿Quieres que haga café?\"),\n",
        "  (\"It is hard to wake up without a strong cup of coffee.\", \"Es difícil despertarse sin una taza de café fuerte.\"),\n",
        "  (\"All I drank was coffee.\", \"Todo lo que bebí fue café.\"),\n",
        "  (\"I've drunk way too much coffee today.\", \"He bebido demasiado café hoy.\"),\n",
        "  (\"Which do you prefer, tea or coffee?\", \"¿Qué prefieres, té o café?\"),\n",
        "  (\"There are many kinds of coffee.\", \"Hay muchas variedades de café.\"),\n",
        "  (\"I will make some coffee.\",\t\"Prepararé algo de café.\")\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}