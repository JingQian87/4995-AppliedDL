{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A4-jq2282.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q-DgS7Ocd4qQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 4 Real-time text classification in the browser\n",
        "### Jing Qian (jq2282)"
      ]
    },
    {
      "metadata": {
        "id": "WG9l9YJ3qYrn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1\n",
        "Modify the starter code (​7-colab-to-webpage.ipynb​) ​to classify snippets of text from four books on ​Project Gutenberg​. \n",
        "\n",
        "Given a snippet of text (not necessarily a complete sentence) predict which book it belongs to.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZSAGAlHbv7d0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1. Data preparation \n",
        "Use method from https://www.nltk.org/book/ch02.html.\n",
        "\n",
        "Get texts from four books on Project Gutenberg: Emma, Paradise, Hamlet and Leaves. \n",
        "\n",
        "Collect a training set with randomly selected 1250 sentences from each book, in which 1000 as training, 200 as validation and 50 as test."
      ]
    },
    {
      "metadata": {
        "id": "OPfR1-rdJY5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c40fc27e-45fc-40bf-abd3-449f2b589a7a"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids ()\n",
        "# test with words form, not use it.\n",
        "emma_words = gutenberg.words('austen-emma.txt')\n",
        "len(emma_words)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "CpyZlsvJlMeY",
        "colab_type": "code",
        "outputId": "4a16ab5b-2fcb-4c34-d689-b501303a0585",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# import books in forms of sentences\n",
        "nltk.download('punkt')\n",
        "emma = gutenberg.sents('austen-emma.txt')\n",
        "paradise = gutenberg.sents('milton-paradise.txt')\n",
        "hamlet = gutenberg.sents('shakespeare-hamlet.txt')\n",
        "leaves = gutenberg.sents('whitman-leaves.txt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o3uDysJPmEVh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "nlines = 1250 \n",
        "emma_pick = np.random.choice(emma, nlines, replace=False)\n",
        "paradise_pick = np.random.choice(paradise, nlines, replace=False)\n",
        "hamlet_pick = np.random.choice(hamlet, nlines, replace=False)\n",
        "leaves_pick = np.random.choice(leaves, nlines, replace=False)\n",
        "xs = np.vstack((emma_pick, paradise_pick, hamlet_pick, leaves_pick))\n",
        "xs = xs.flatten()\n",
        "#label 0:emma, label 1:paradise, 2:hamlet, 3:leaves\n",
        "ys = [0]*nlines + [1]*nlines + [2]*nlines + [3]*nlines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q8YT2qJXF5f6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c611ee0-923b-46ea-ca10-b73f7e598c99"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "#shuffle is True defaultly, before split\n",
        "x_train, x_vt, y_train, y_vt = train_test_split(xs, ys, test_size=0.2)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_vt, y_vt, test_size=0.2)\n",
        "print(len(y_train), len(y_val), len(y_test))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000 800 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ha3q3HcHkjNX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0441bb2f-7cac-4ff3-affa-27d1a2a13231"
      },
      "cell_type": "code",
      "source": [
        "max_len = 20\n",
        "num_words = 10000\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# Fit the tokenizer on the training data\n",
        "t = Tokenizer(num_words=num_words)\n",
        "t.fit_on_texts(x_train)\n",
        "#print(t.word_index)\n",
        "vectorized = t.texts_to_sequences(emma_pick)\n",
        "#print(vectorized)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "padded = pad_sequences(vectorized, maxlen=max_len, padding='post')\n",
        "#print(padded)\n",
        "\n",
        "metadata = {\n",
        "  'word_index': t.word_index,\n",
        "  'max_len': max_len,\n",
        "  'vocabulary_size': num_words,\n",
        "}\n",
        "\n",
        "x_train = t.texts_to_sequences(x_train)\n",
        "x_train = pad_sequences(x_train, maxlen=max_len, padding='post')\n",
        "# print(x_train)\n",
        "x_val = t.texts_to_sequences(x_val)\n",
        "x_val = pad_sequences(x_val, maxlen=max_len, padding='post')\n",
        "# x_test = t.texts_to_sequences(x_test)\n",
        "# x_test = pad_sequences(x_test, maxlen=max_len, padding='post')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "LtTJGE_QxX27",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 4. Define a model, train and test"
      ]
    },
    {
      "metadata": {
        "id": "NLSxcbnWkszT",
        "colab_type": "code",
        "outputId": "6558ef06-1891-461e-89b0-91235b57cd1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "cell_type": "code",
      "source": [
        "embedding_size = 8\n",
        "n_classes = 4\n",
        "epochs = 10\n",
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(num_words, embedding_size, input_shape=(max_len,)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(4, activation='softmax'))\n",
        "model.compile('adam', 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 20, 8)             80000     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 160)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 644       \n",
            "=================================================================\n",
            "Total params: 80,644\n",
            "Trainable params: 80,644\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qaxPLGFvks6s",
        "colab_type": "code",
        "outputId": "9d9577f6-33ff-4fab-aa8f-a72de4a60d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=10)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4000 samples, validate on 800 samples\n",
            "Epoch 1/10\n",
            "4000/4000 [==============================] - 1s 188us/sample - loss: 1.3411 - acc: 0.3733 - val_loss: 1.2508 - val_acc: 0.4338\n",
            "Epoch 2/10\n",
            "4000/4000 [==============================] - 0s 81us/sample - loss: 1.1647 - acc: 0.4905 - val_loss: 1.0919 - val_acc: 0.5275\n",
            "Epoch 3/10\n",
            "4000/4000 [==============================] - 0s 82us/sample - loss: 0.9802 - acc: 0.6352 - val_loss: 0.9298 - val_acc: 0.6675\n",
            "Epoch 4/10\n",
            "4000/4000 [==============================] - 0s 81us/sample - loss: 0.7718 - acc: 0.7847 - val_loss: 0.7715 - val_acc: 0.7575\n",
            "Epoch 5/10\n",
            "4000/4000 [==============================] - 0s 86us/sample - loss: 0.5860 - acc: 0.8545 - val_loss: 0.6520 - val_acc: 0.7775\n",
            "Epoch 6/10\n",
            "4000/4000 [==============================] - 0s 86us/sample - loss: 0.4465 - acc: 0.9057 - val_loss: 0.5679 - val_acc: 0.8100\n",
            "Epoch 7/10\n",
            "4000/4000 [==============================] - 0s 79us/sample - loss: 0.3452 - acc: 0.9325 - val_loss: 0.5102 - val_acc: 0.8250\n",
            "Epoch 8/10\n",
            "4000/4000 [==============================] - 0s 79us/sample - loss: 0.2721 - acc: 0.9480 - val_loss: 0.4668 - val_acc: 0.8425\n",
            "Epoch 9/10\n",
            "4000/4000 [==============================] - 0s 81us/sample - loss: 0.2186 - acc: 0.9638 - val_loss: 0.4352 - val_acc: 0.8525\n",
            "Epoch 10/10\n",
            "4000/4000 [==============================] - 0s 77us/sample - loss: 0.1786 - acc: 0.9688 - val_loss: 0.4113 - val_acc: 0.8575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dJJolMoe8e85",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#待改动的test"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b4264afa-e978-4bdf-a3db-fbe8d8e1beac",
        "id": "Wvq-dhOA8bZ0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "test_example = paradise[1200]\n",
        "x_test = t.texts_to_sequences([test_example])\n",
        "x_test = pad_sequences(x_test, maxlen=max_len, padding='post')\n",
        "print(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 81   1   1   8 126  12 126  26  65   1  65  14   7  25  34   0   0   0\n",
            "    0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UYoMPP8Vks-N",
        "colab_type": "code",
        "outputId": "911cab54-0a8d-4d09-9e64-7e7caed21d47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "preds = model.predict(x_test)\n",
        "print(preds)\n",
        "import numpy as np\n",
        "print(np.argmax(preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.24311227 0.26148778 0.24880302 0.24659693]]\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TgsuXiPAEf6R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##挪到下面"
      ]
    },
    {
      "metadata": {
        "id": "5kTVHyQxvDu6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 1. Environment Preparation"
      ]
    },
    {
      "metadata": {
        "id": "0FAw9Qa0akC6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "olI7l_bmcCMk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflowjs==1.0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cNILWuVrvLJD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Connet to the github page, use the applied-dl repository: https://github.com/fakeJQ/applied-dl. \n",
        "\n",
        "Generated page of the repository, shown as: https://fakejq.github.io/applied-dl/"
      ]
    },
    {
      "metadata": {
        "id": "8jnxQeiQxF3C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2. Copy repository from Github"
      ]
    },
    {
      "metadata": {
        "id": "007TFGrqeHq9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# your github username\n",
        "USER_NAME = \"fakeJQ\" \n",
        "\n",
        "# the email associated with your commits\n",
        "# (may not matter if you leave it as is)\n",
        "USER_EMAIL = \"tcqj_8758@163.com\" \n",
        "\n",
        "# create a token by visiting https://github.com/settings/tokens\n",
        "# choose public permissions\n",
        "# important: treat this token like a password (do not commit it)\n",
        "# or submit it w/ your HW.\n",
        "TOKEN = \"89d35eaae4bb746d07f304526f39807c2c267f5c\" \n",
        "\n",
        "# for example, if your user_name is \"foo\", then this notebook will create\n",
        "# a site at \"https://foo.github.io/hw4/\"\n",
        "SITE_NAME = \"hw4\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mAF4WYFDeSqj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!git config --global user.email {USER_NAME}\n",
        "!git config --global user.name  {USER_EMAIL}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ykgeRmHeYRR",
        "colab_type": "code",
        "outputId": "e0a93a1e-897d-444c-9d1d-9e51df248aad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "repo_path = USER_NAME + '.github.io'\n",
        "if not os.path.exists(os.path.join(os.getcwd(), repo_path)):\n",
        "  !git clone https://{USER_NAME}:{TOKEN}@github.com/{USER_NAME}/applied-dl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'applied-dl' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7oDw9vhZihwK",
        "colab_type": "code",
        "outputId": "8a2da844-8459-4e27-9d14-4f5e6fa3518b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "os.chdir('/content/applied-dl')\n",
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UQ3thYvLjwAf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "project_path = os.path.join(os.getcwd(), SITE_NAME)\n",
        "if not os.path.exists(project_path): \n",
        "  os.mkdir(project_path)\n",
        "os.chdir(project_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y33fXMHYjwD0",
        "colab_type": "code",
        "outputId": "af732027-db19-4d5d-9a7f-a5d7f4f9ce4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(project_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/applied-dl/hw4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DoRtckGSkVK1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# DO NOT MODIFY\n",
        "MODEL_DIR = os.path.join(project_path, \"model_js\")\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "katbomi_xkZK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 5. Goto html!"
      ]
    },
    {
      "metadata": {
        "id": "JUfiXSXkk8cA",
        "colab_type": "code",
        "outputId": "a37df651-0841-4c59-b25c-87164e134663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tensorflowjs as tfjs\n",
        "\n",
        "metadata_json_path = os.path.join(MODEL_DIR, 'metadata.json')\n",
        "json.dump(metadata, open(metadata_json_path, 'wt'))\n",
        "tfjs.converters.save_keras_model(model, MODEL_DIR)\n",
        "print('\\nSaved model artifcats in directory: %s' % MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Saved model artifcats in directory: /content/applied-dl/hw4/model_js\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RAIBwRjzk8fQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_html = \"\"\"\n",
        "<!doctype html>\n",
        "\n",
        "<body>\n",
        "  <style>\n",
        "    #textfield {\n",
        "      font-size: 120%;\n",
        "      width: 60%;\n",
        "      height: 200px;\n",
        "    }\n",
        "  </style>\n",
        "  <h1>\n",
        "    Title\n",
        "  </h1>\n",
        "  <hr>\n",
        "  <div class=\"create-model\">\n",
        "    <button id=\"load-model\" style=\"display:none\">Load model</button>\n",
        "  </div>\n",
        "  <div>\n",
        "    <div>\n",
        "      <span>Vocabulary size: </span>\n",
        "      <span id=\"vocabularySize\"></span>\n",
        "    </div>\n",
        "    <div>\n",
        "      <span>Max length: </span>\n",
        "      <span id=\"maxLen\"></span>\n",
        "    </div>\n",
        "  </div>\n",
        "  <hr>\n",
        "  <div>\n",
        "    <select id=\"example-select\" class=\"form-control\">\n",
        "      <option value=\"example1\">Alice's Adventures in Wonderland</option>\n",
        "      <option value=\"example2\">Dracula</option>\n",
        "      <option value=\"example3\">The Iliad</option>\n",
        "    </select>\n",
        "  </div>\n",
        "  <div>\n",
        "    <textarea id=\"text-entry\"></textarea>\n",
        "  </div>\n",
        "  <hr>\n",
        "  <div>\n",
        "    <span id=\"status\">Standing by.</span>\n",
        "  </div>\n",
        "\n",
        "  <script src='https://cdn.jsdelivr.net/npm/@tensorflow/tfjs/dist/tf.min.js'></script>\n",
        "  <script src='index.js'></script>\n",
        "</body>\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ut7T6FdQk8kw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index_js = \"\"\"\n",
        "const HOSTED_URLS = {\n",
        "  model:\n",
        "      'model_js/model.json',\n",
        "  metadata:\n",
        "      'model_js/metadata.json'\n",
        "};\n",
        "\n",
        "const examples = {\n",
        "  'example1':\n",
        "      'Alice was beginning to get very tired of sitting by her sister on the bank.',\n",
        "  'example2':\n",
        "      'Buda-Pesth seems a wonderful place.',\n",
        "  'example3':\n",
        "      'Scepticism was as much the result of knowledge, as knowledge is of scepticism.'      \n",
        "};\n",
        "\n",
        "function status(statusText) {\n",
        "  console.log(statusText);\n",
        "  document.getElementById('status').textContent = statusText;\n",
        "}\n",
        "\n",
        "function showMetadata(metadataJSON) {\n",
        "  document.getElementById('vocabularySize').textContent =\n",
        "      metadataJSON['vocabulary_size'];\n",
        "  document.getElementById('maxLen').textContent =\n",
        "      metadataJSON['max_len'];\n",
        "}\n",
        "\n",
        "function settextField(text, predict) {\n",
        "  const textField = document.getElementById('text-entry');\n",
        "  textField.value = text;\n",
        "  doPredict(predict);\n",
        "}\n",
        "\n",
        "function setPredictFunction(predict) {\n",
        "  const textField = document.getElementById('text-entry');\n",
        "  textField.addEventListener('input', () => doPredict(predict));\n",
        "}\n",
        "\n",
        "function disableLoadModelButtons() {\n",
        "  document.getElementById('load-model').style.display = 'none';\n",
        "}\n",
        "\n",
        "function doPredict(predict) {\n",
        "  const textField = document.getElementById('text-entry');\n",
        "  const result = predict(textField.value);\n",
        "  score_string = \"Class scores: \";\n",
        "  for (var x in result.score) {\n",
        "    score_string += x + \" ->  \" + result.score[x].toFixed(3) + \", \"\n",
        "  }\n",
        "  //console.log(score_string);\n",
        "  status(\n",
        "      score_string + ' elapsed: ' + result.elapsed.toFixed(3) + ' ms)');\n",
        "}\n",
        "\n",
        "function prepUI(predict) {\n",
        "  setPredictFunction(predict);\n",
        "  const testExampleSelect = document.getElementById('example-select');\n",
        "  testExampleSelect.addEventListener('change', () => {\n",
        "    settextField(examples[testExampleSelect.value], predict);\n",
        "  });\n",
        "  settextField(examples['example1'], predict);\n",
        "}\n",
        "\n",
        "async function urlExists(url) {\n",
        "  status('Testing url ' + url);\n",
        "  try {\n",
        "    const response = await fetch(url, {method: 'HEAD'});\n",
        "    return response.ok;\n",
        "  } catch (err) {\n",
        "    return false;\n",
        "  }\n",
        "}\n",
        "\n",
        "async function loadHostedPretrainedModel(url) {\n",
        "  status('Loading pretrained model from ' + url);\n",
        "  try {\n",
        "    const model = await tf.loadLayersModel(url);\n",
        "    status('Done loading pretrained model.');\n",
        "    disableLoadModelButtons();\n",
        "    return model;\n",
        "  } catch (err) {\n",
        "    console.error(err);\n",
        "    status('Loading pretrained model failed.');\n",
        "  }\n",
        "}\n",
        "\n",
        "async function loadHostedMetadata(url) {\n",
        "  status('Loading metadata from ' + url);\n",
        "  try {\n",
        "    const metadataJson = await fetch(url);\n",
        "    const metadata = await metadataJson.json();\n",
        "    status('Done loading metadata.');\n",
        "    return metadata;\n",
        "  } catch (err) {\n",
        "    console.error(err);\n",
        "    status('Loading metadata failed.');\n",
        "  }\n",
        "}\n",
        "\n",
        "class Classifier {\n",
        "\n",
        "  async init(urls) {\n",
        "    this.urls = urls;\n",
        "    this.model = await loadHostedPretrainedModel(urls.model);\n",
        "    await this.loadMetadata();\n",
        "    return this;\n",
        "  }\n",
        "\n",
        "  async loadMetadata() {\n",
        "    const metadata =\n",
        "        await loadHostedMetadata(this.urls.metadata);\n",
        "    showMetadata(metadata);\n",
        "    this.maxLen = metadata['max_len'];\n",
        "    console.log('maxLen = ' + this.maxLen);\n",
        "    this.wordIndex = metadata['word_index']\n",
        "  }\n",
        "\n",
        "  predict(text) {\n",
        "    // Convert to lower case and remove all punctuations.\n",
        "    const inputText =\n",
        "        text.trim().toLowerCase().replace(/(\\.|\\,|\\!)/g, '').split(' ');\n",
        "    // Look up word indices.\n",
        "    const inputBuffer = tf.buffer([1, this.maxLen], 'float32');\n",
        "    for (let i = 0; i < inputText.length; ++i) {\n",
        "      const word = inputText[i];\n",
        "      inputBuffer.set(this.wordIndex[word], 0, i);\n",
        "      //console.log(word, this.wordIndex[word], inputBuffer);\n",
        "    }\n",
        "    const input = inputBuffer.toTensor();\n",
        "    //console.log(input);\n",
        "\n",
        "    status('Running inference');\n",
        "    const beginMs = performance.now();\n",
        "    const predictOut = this.model.predict(input);\n",
        "    //console.log(predictOut.dataSync());\n",
        "    const score = predictOut.dataSync();//[0];\n",
        "    predictOut.dispose();\n",
        "    const endMs = performance.now();\n",
        "\n",
        "    return {score: score, elapsed: (endMs - beginMs)};\n",
        "  }\n",
        "};\n",
        "\n",
        "async function setup() {\n",
        "  if (await urlExists(HOSTED_URLS.model)) {\n",
        "    status('Model available: ' + HOSTED_URLS.model);\n",
        "    const button = document.getElementById('load-model');\n",
        "    button.addEventListener('click', async () => {\n",
        "      const predictor = await new Classifier().init(HOSTED_URLS);\n",
        "      prepUI(x => predictor.predict(x));\n",
        "    });\n",
        "    button.style.display = 'inline-block';\n",
        "  }\n",
        "\n",
        "  status('Standing by.');\n",
        "}\n",
        "\n",
        "setup();\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ycCV-t1Fk8n9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('index.html','w') as f:\n",
        "  f.write(index_html)\n",
        "  \n",
        "with open('index.js','w') as f:\n",
        "  f.write(index_js)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdDhUwQ5k8rF",
        "colab_type": "code",
        "outputId": "96f93463-6877-42d2-809f-179334f64e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "index.html  index.js  model_js\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yQeTiua_k8ua",
        "colab_type": "code",
        "outputId": "c3649d9e-b6a1-49a2-a61f-dd25797276cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "cell_type": "code",
      "source": [
        "#!git add . \n",
        "#!git commit -m \"colab -> github\"\n",
        "!git push https://{USER_NAME}:{TOKEN}@github.com/{USER_NAME}/applied-dl.git master\n",
        "#!git push https://{USER_NAME}:{TOKEN}@github.com/{USER_NAME}/applied-dl"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counting objects: 9, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  11% (1/9)   \rCompressing objects:  22% (2/9)   \rCompressing objects:  33% (3/9)   \rCompressing objects:  44% (4/9)   \rCompressing objects:  55% (5/9)   \rCompressing objects:  66% (6/9)   \rCompressing objects:  77% (7/9)   \rCompressing objects:  88% (8/9)   \rCompressing objects: 100% (9/9)   \rCompressing objects: 100% (9/9), done.\n",
            "Writing objects:  11% (1/9)   \rWriting objects:  22% (2/9)   \rWriting objects:  33% (3/9)   \rWriting objects:  44% (4/9)   \rWriting objects:  55% (5/9)   \rWriting objects:  66% (6/9)   \rWriting objects:  77% (7/9)   \rWriting objects:  88% (8/9)   \rWriting objects: 100% (9/9)   \rWriting objects: 100% (9/9), 34.15 KiB | 11.38 MiB/s, done.\n",
            "Total 9 (delta 1), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
            "To https://github.com/fakeJQ/applied-dl.git\n",
            "   f116369..0804b74  master -> master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2QuvjTM-k8iG",
        "colab_type": "code",
        "outputId": "0fa432a8-6ccb-46e8-f4ce-1bc6c235a0a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Now, visit https://%s.github.io/applied-dl/%s/\" % (USER_NAME, SITE_NAME))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Now, visit https://fakeJQ.github.io/applied-dl/hw4/\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}