{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruU23naDpRx6",
        "colab_type": "text"
      },
      "source": [
        "# 4998 Applied Deep Learning Project\n",
        "## Detect Cancer Metastases on Pathology Images\n",
        "\n",
        "Jing Qian (jq2282)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiEGCtNYp05o",
        "colab_type": "text"
      },
      "source": [
        "# STEP 1. IMAGE PROCESSING\n",
        "\n",
        "* Read this paper to understand their approach https://arxiv.org/pdf/1703.02442.pdf\n",
        "* Start small (use a single slide, at the lowest available zoom level - the effective resolution should be on the order of 1,000 by 1,000 pixels).\n",
        "* Write code to slide a window across the slide. Extract patches and labels (using the tissue mask)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsBP4bwrv6bY",
        "colab_type": "text"
      },
      "source": [
        "## 1.1. Import data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1cvzVAev9iy",
        "colab_type": "code",
        "outputId": "c2a6428c-991e-4477-bec3-3079d9617914",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Install the OpenSlide C library and Python bindings\n",
        "!apt-get install openslide-tools\n",
        "!pip install openslide-python"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openslide-tools is already the newest version (3.4.1+dfsg-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Requirement already satisfied: openslide-python in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from openslide-python) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->openslide-python) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cQEzHKUwH3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from openslide import open_slide, __library_version__ as openslide_version\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2gray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or8nAcNS5gIw",
        "colab_type": "code",
        "outputId": "3e08c79a-ef99-4d98-f7dd-9fb0acbdfe55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9LcDH77fDL",
        "colab_type": "code",
        "outputId": "601f7bc7-a022-4347-bfd5-64fe3c4686ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# list all the slides provided by professor\n",
        "!ls '/content/drive/My Drive/slides'"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " tumor_001_mask.tif   tumor_031.xml\t   tumor_081.tif\n",
            " tumor_001.tif\t      tumor_035_mask.tif   tumor_081.xml\n",
            " tumor_001.xml\t      tumor_035.tif\t   tumor_084_mask.tif\n",
            " tumor_002_mask.tif   tumor_035.xml\t   tumor_084.tif\n",
            " tumor_002.tif\t     'tumor_038 (1).xml'   tumor_084.xml\n",
            " tumor_002.xml\t      tumor_038.tif\t   tumor_091_mask.tif\n",
            " tumor_005_mask.tif   tumor_038.xml\t   tumor_091.tif\n",
            " tumor_005.tif\t      tumor_057_mask.tif   tumor_091.xml\n",
            " tumor_005.xml\t      tumor_057.tif\t   tumor_094_mask.tif\n",
            " tumor_012_mask.tif   tumor_057.xml\t   tumor_094.tif\n",
            " tumor_012.tif\t      tumor_059_mask.tif   tumor_094.xml\n",
            " tumor_012.xml\t      tumor_059.tif\t   tumor_096_mask.tif\n",
            " tumor_016_mask.tif   tumor_059.xml\t   tumor_096.tif\n",
            " tumor_016.tif\t      tumor_064_mask.tif   tumor_096.xml\n",
            " tumor_016.xml\t      tumor_064.tif\t   tumor_099.xml\n",
            " tumor_019_mask.tif   tumor_064.xml\t   tumor_101_mask.tif\n",
            " tumor_019.tif\t      tumor_075_mask.tif   tumor_101.tif\n",
            " tumor_019.xml\t      tumor_075.tif\t   tumor_101.xml\n",
            " tumor_023_mask.tif   tumor_075.xml\t   tumor_110_mask.tif\n",
            " tumor_023.tif\t      tumor_078_mask.tif   tumor_110.tif\n",
            " tumor_023.xml\t      tumor_078.tif\t   tumor_110.xml\n",
            " tumor_031_mask.tif   tumor_078.xml\n",
            " tumor_031.tif\t      tumor_081_mask.tif\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeI49ONIdcHq",
        "colab_type": "text"
      },
      "source": [
        "从上面可以看到完整的图像有21个，tumor_038和tumor_099有问题，不用。如果是全部都用于train/test，就17个train, 4个test。从1 vs 1开始"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mO14LiMAU0d7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "slides_list = ['001','002','005','012','016','019','023','031','035','057','059',\n",
        "              '064','075','078','081','084','091','094','096','101','110']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB8BE5I9Q3gy",
        "colab_type": "text"
      },
      "source": [
        "## 1.2. Generate training patches and corresponding labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_38-ciKBtOP",
        "colab_type": "text"
      },
      "source": [
        "### Start with one slide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RL5-TRZQF-46",
        "colab_type": "text"
      },
      "source": [
        "###  General Params"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWy9EgLeGCOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 299\n",
        "tumor_check_size = 128\n",
        "ilevel = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ZIwbxyCNWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_path = '/content/drive/My Drive/project-adl/slides'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASxFrOl_iXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See https://openslide.org/api/python/#openslide.OpenSlide.read_region\n",
        "# Note: x,y coords are with respect to level 0.\n",
        "# There is an example below of working with coordinates\n",
        "# with respect to a higher zoom level.\n",
        "\n",
        "# Read a region from the slide\n",
        "# Return a numpy RBG array\n",
        "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
        "    im = slide.read_region((x,y), level, (width, height))\n",
        "    im = im.convert('RGB') # drop the alpha channel\n",
        "    if as_float:\n",
        "        im = np.asarray(im, dtype=np.float32)\n",
        "    else:\n",
        "        im = np.asarray(im)\n",
        "    assert im.shape == (height, width, 3)\n",
        "    return im"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o6-f4hX6HKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n",
        "# of the slide. We'll find these by looking for all gray regions.\n",
        "def find_tissue_pixels(image, intensity=0.8):\n",
        "    im_gray = rgb2gray(image)\n",
        "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
        "    indices = np.where(im_gray <= intensity)\n",
        "    return zip(indices[0], indices[1])\n",
        "\n",
        "# This function has nothing to do with tumor mask, but shows tissue region\n",
        "def apply_mask(im, mask, color=(255,0,0)):\n",
        "    masked = np.copy(im)\n",
        "    for x,y in mask: masked[x][y] = color\n",
        "    return masked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp0i_0o25KIg",
        "colab_type": "code",
        "outputId": "c3558f7a-d4f8-4290-c0cf-ac2442815378",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "slide_id = '091'\n",
        "slide_path = os.path.join(total_path,'tumor_%s.tif' %slide_id)\n",
        "tumor_mask_path = os.path.join(total_path,'tumor_%s_mask.tif' %slide_id)\n",
        "\n",
        "slide = open_slide(slide_path)\n",
        "print (\"Read WSI from %s with width: %d, height: %d\" % (slide_path, \n",
        "                                                        slide.level_dimensions[0][0], \n",
        "                                                        slide.level_dimensions[0][1]))\n",
        "\n",
        "tumor_mask = open_slide(tumor_mask_path)\n",
        "print (\"Read tumor mask from %s\" % (tumor_mask_path))\n",
        "\n",
        "print(\"Slide includes %d levels\", len(slide.level_dimensions))\n",
        "for i in range(len(slide.level_dimensions)):\n",
        "    print(\"Level %d, dimensions: %s downsample factor %d\" % (i, \n",
        "                                                             slide.level_dimensions[i], \n",
        "                                                             slide.level_downsamples[i]))\n",
        "    assert tumor_mask.level_dimensions[i][0] == slide.level_dimensions[i][0]\n",
        "    assert tumor_mask.level_dimensions[i][1] == slide.level_dimensions[i][1]\n",
        "\n",
        "# Verify downsampling works as expected\n",
        "width, height = slide.level_dimensions[7]\n",
        "assert width * slide.level_downsamples[7] == slide.level_dimensions[0][0]\n",
        "assert height * slide.level_downsamples[7] == slide.level_dimensions[0][1]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read WSI from /content/drive/My Drive/project-adl/slides/tumor_091.tif with width: 61440, height: 53760\n",
            "Read tumor mask from /content/drive/My Drive/project-adl/slides/tumor_091_mask.tif\n",
            "Slide includes %d levels 8\n",
            "Level 0, dimensions: (61440, 53760) downsample factor 1\n",
            "Level 1, dimensions: (30720, 26880) downsample factor 2\n",
            "Level 2, dimensions: (15360, 13440) downsample factor 4\n",
            "Level 3, dimensions: (7680, 6720) downsample factor 8\n",
            "Level 4, dimensions: (3840, 3360) downsample factor 16\n",
            "Level 5, dimensions: (1920, 1680) downsample factor 32\n",
            "Level 6, dimensions: (960, 840) downsample factor 64\n",
            "Level 7, dimensions: (480, 420) downsample factor 128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oS4JwEFPkZK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example: read the entire slide at level 4\n",
        "\n",
        "# Higher zoom levels may not fit into memory.\n",
        "# You can use the below function to extract regions from higher zoom levels \n",
        "# without having to read the entire image into ram.\n",
        "\n",
        "# Use the sliding window approach discussed in class to collect training\n",
        "# data for your classifier. E.g., slide a window across the slide (for\n",
        "# starters, use a zoomed out view, so you're not working with giant images).\n",
        "# Save each window to disk as an image. To find the label for that image, \n",
        "# check to the tissue mask to see if the same region contains cancerous cells.\n",
        "\n",
        "# Important: this is tricky to get right. Carefully debug your pipeline before\n",
        "# training your model. Start with just a single image, and a relatively \n",
        "# low zoom level.\n",
        "\n",
        "slide_image = read_slide(slide, \n",
        "                         x=0, \n",
        "                         y=0, \n",
        "                         level=ilevel, \n",
        "                         width=slide.level_dimensions[ilevel][0], \n",
        "                         height=slide.level_dimensions[ilevel][1])\n",
        "\n",
        "#plt.figure(figsize=(10,10), dpi=100)\n",
        "#plt.imshow(slide_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqGoWFNBkWiW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5ff0e4e2-ba45-4cc1-ed37-7352b65eedcb"
      },
      "source": [
        "# Keep only tissue regions\n",
        "tissue_pixels = list(find_tissue_pixels(slide_image))\n",
        "print('Dimension of Tissue_pixels:', np.shape(tissue_pixels))\n",
        "percent_tissue = len(tissue_pixels) / float(slide_image.shape[0] * slide_image.shape[0]) * 100\n",
        "print (\"%d tissue_pixels pixels (%.1f percent of the image)\" % (len(tissue_pixels), percent_tissue)) \n",
        "\n",
        "#tissue_regions = apply_mask(slide_image, tissue_pixels)\n",
        "#plt.imshow(tissue_regions)\n",
        "#plt.imshow(mask_image,alpha=0.5)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimension of Tissue_pixels: (3451212, 2)\n",
            "3451212 tissue_pixels pixels (30.6 percent of the image)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMNGMgeS_iT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6a61fb8d-0dd9-404e-ce69-67fc6ca7a9c5"
      },
      "source": [
        "xmin, xmax, ymin, ymax = 0, 0, 0, 0\n",
        "for i in tissue_pixels:\n",
        "  xmin = min(i[0], xmin) #x correspond to dimention[\n",
        "  xmax = max(i[0], xmax)\n",
        "  ymin = min(i[1], ymin)\n",
        "  ymax = max(i[1], ymax)\n",
        "  \n",
        "print(xmin, xmax, ymin, ymax)\n",
        "print('canvas size:', slide.level_dimensions[ilevel])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3359 0 3839\n",
            "canvas size: (3840, 3360)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF_e1zAEPByB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bound = (input_size - 1)/2\n",
        "nclass = 10\n",
        "train_0, train_1 = [], []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-BucaT-7trc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6511
        },
        "outputId": "bdf47132-6a23-4e62-ec4f-7886c6acd220"
      },
      "source": [
        "while len(train_0) < nclass or len(train_1) < nclass:\n",
        "  # Random sample a tissue pixel and generate a patch centered in this pixel\n",
        "  while True:\n",
        "    center = tissue_pixels[np.random.choice(np.shape(tissue_pixels)[0])]\n",
        "    if center[0] >= bound and center[0] <= slide.level_dimensions[ilevel][1]-bound:\n",
        "      if center[1] >= bound and center[1] <= slide.level_dimensions[ilevel][0]-bound:\n",
        "        break\n",
        "  print(center)\n",
        "\n",
        "  # Expand a 299*299 patch around the center tissue\n",
        "  ipatch = read_slide(slide, \n",
        "                     x=int(center[0]-bound), \n",
        "                     y=int(center[1]-bound), \n",
        "                     level=ilevel, \n",
        "                     width=input_size, \n",
        "                     height=input_size)\n",
        "\n",
        "  # Check whether the 128*128 center pixels have tumor\n",
        "  ipixels = read_slide(tumor_mask,\n",
        "                       x = center[0]*16-tumor_check_size//2, #need to change 16 for other levels\n",
        "                       y = center[1]*16-tumor_check_size//2,\n",
        "                       level = ilevel,\n",
        "                       width = tumor_check_size,\n",
        "                       height = tumor_check_size)[:,:,0]\n",
        "  #plt.imshow(ipixels)\n",
        "\n",
        "  ilabel = 0\n",
        "  if np.any(ipixels) == 1:\n",
        "    ilabel = 1\n",
        "  print('The label for the patch is:', str(ilabel))\n",
        "  if ilabel == 0:\n",
        "    if len(train_0) < nclass:\n",
        "      train_0.append(ipatch)\n",
        "      print('train_0 add one patch and current have %d patches!' %len(train_0))\n",
        "  else:\n",
        "    if len(train_1) < nclass:\n",
        "      train_1.append(ipatch)\n",
        "      print('train_1 add one patch and current have %d patches!' %len(train_1))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(991, 2022)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 1 patches!\n",
            "(1225, 1451)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 2 patches!\n",
            "(1606, 1265)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 3 patches!\n",
            "(1875, 2918)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 4 patches!\n",
            "(813, 1621)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 5 patches!\n",
            "(1387, 3191)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 6 patches!\n",
            "(1001, 1239)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 7 patches!\n",
            "(2813, 2667)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 8 patches!\n",
            "(1701, 1382)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 9 patches!\n",
            "(983, 1393)\n",
            "The label for the patch is: 0\n",
            "train_0 add one patch and current have 10 patches!\n",
            "(2021, 1223)\n",
            "The label for the patch is: 0\n",
            "(698, 2520)\n",
            "The label for the patch is: 0\n",
            "(1417, 2178)\n",
            "The label for the patch is: 0\n",
            "(1524, 1362)\n",
            "The label for the patch is: 0\n",
            "(1182, 3123)\n",
            "The label for the patch is: 0\n",
            "(1740, 1967)\n",
            "The label for the patch is: 0\n",
            "(2590, 2329)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 1 patches!\n",
            "(1746, 2163)\n",
            "The label for the patch is: 0\n",
            "(1556, 3211)\n",
            "The label for the patch is: 0\n",
            "(2017, 2936)\n",
            "The label for the patch is: 0\n",
            "(1735, 2549)\n",
            "The label for the patch is: 0\n",
            "(1473, 2768)\n",
            "The label for the patch is: 0\n",
            "(1958, 1536)\n",
            "The label for the patch is: 0\n",
            "(1635, 2468)\n",
            "The label for the patch is: 0\n",
            "(406, 2241)\n",
            "The label for the patch is: 0\n",
            "(1722, 3003)\n",
            "The label for the patch is: 0\n",
            "(811, 1868)\n",
            "The label for the patch is: 0\n",
            "(1675, 1317)\n",
            "The label for the patch is: 0\n",
            "(2398, 2644)\n",
            "The label for the patch is: 0\n",
            "(1765, 3154)\n",
            "The label for the patch is: 0\n",
            "(1275, 3099)\n",
            "The label for the patch is: 0\n",
            "(1741, 2678)\n",
            "The label for the patch is: 0\n",
            "(2250, 1905)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 2 patches!\n",
            "(1184, 3121)\n",
            "The label for the patch is: 0\n",
            "(677, 2122)\n",
            "The label for the patch is: 0\n",
            "(2294, 2048)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 3 patches!\n",
            "(1617, 2584)\n",
            "The label for the patch is: 0\n",
            "(1619, 2131)\n",
            "The label for the patch is: 0\n",
            "(1831, 1618)\n",
            "The label for the patch is: 0\n",
            "(804, 1598)\n",
            "The label for the patch is: 0\n",
            "(1538, 1372)\n",
            "The label for the patch is: 0\n",
            "(1933, 587)\n",
            "The label for the patch is: 0\n",
            "(709, 1549)\n",
            "The label for the patch is: 0\n",
            "(1628, 2282)\n",
            "The label for the patch is: 0\n",
            "(1633, 2119)\n",
            "The label for the patch is: 0\n",
            "(1405, 2564)\n",
            "The label for the patch is: 0\n",
            "(2245, 2410)\n",
            "The label for the patch is: 0\n",
            "(1956, 2266)\n",
            "The label for the patch is: 0\n",
            "(1537, 2721)\n",
            "The label for the patch is: 0\n",
            "(1705, 2157)\n",
            "The label for the patch is: 0\n",
            "(2862, 2623)\n",
            "The label for the patch is: 0\n",
            "(1116, 2905)\n",
            "The label for the patch is: 0\n",
            "(1045, 2495)\n",
            "The label for the patch is: 0\n",
            "(2334, 1676)\n",
            "The label for the patch is: 0\n",
            "(1783, 1501)\n",
            "The label for the patch is: 0\n",
            "(1056, 2933)\n",
            "The label for the patch is: 0\n",
            "(2579, 1570)\n",
            "The label for the patch is: 0\n",
            "(2350, 2487)\n",
            "The label for the patch is: 0\n",
            "(2014, 2043)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 4 patches!\n",
            "(2338, 2574)\n",
            "The label for the patch is: 0\n",
            "(1396, 2791)\n",
            "The label for the patch is: 0\n",
            "(2648, 1405)\n",
            "The label for the patch is: 0\n",
            "(1364, 2150)\n",
            "The label for the patch is: 0\n",
            "(1480, 3188)\n",
            "The label for the patch is: 0\n",
            "(727, 1863)\n",
            "The label for the patch is: 0\n",
            "(1755, 805)\n",
            "The label for the patch is: 0\n",
            "(1378, 939)\n",
            "The label for the patch is: 0\n",
            "(812, 2452)\n",
            "The label for the patch is: 0\n",
            "(965, 1718)\n",
            "The label for the patch is: 0\n",
            "(1870, 2836)\n",
            "The label for the patch is: 0\n",
            "(676, 2195)\n",
            "The label for the patch is: 0\n",
            "(1754, 2698)\n",
            "The label for the patch is: 0\n",
            "(1971, 3045)\n",
            "The label for the patch is: 0\n",
            "(2402, 1701)\n",
            "The label for the patch is: 0\n",
            "(1458, 2737)\n",
            "The label for the patch is: 0\n",
            "(1384, 936)\n",
            "The label for the patch is: 0\n",
            "(1793, 2429)\n",
            "The label for the patch is: 0\n",
            "(1491, 3175)\n",
            "The label for the patch is: 0\n",
            "(1942, 1738)\n",
            "The label for the patch is: 0\n",
            "(2025, 673)\n",
            "The label for the patch is: 0\n",
            "(464, 2142)\n",
            "The label for the patch is: 0\n",
            "(1958, 1731)\n",
            "The label for the patch is: 0\n",
            "(1238, 1100)\n",
            "The label for the patch is: 0\n",
            "(2037, 2723)\n",
            "The label for the patch is: 0\n",
            "(761, 1377)\n",
            "The label for the patch is: 0\n",
            "(862, 1703)\n",
            "The label for the patch is: 0\n",
            "(1862, 2457)\n",
            "The label for the patch is: 0\n",
            "(960, 2713)\n",
            "The label for the patch is: 0\n",
            "(1341, 3083)\n",
            "The label for the patch is: 0\n",
            "(2305, 2574)\n",
            "The label for the patch is: 0\n",
            "(1406, 814)\n",
            "The label for the patch is: 0\n",
            "(1302, 2253)\n",
            "The label for the patch is: 0\n",
            "(1172, 1951)\n",
            "The label for the patch is: 0\n",
            "(1518, 2283)\n",
            "The label for the patch is: 0\n",
            "(1716, 1212)\n",
            "The label for the patch is: 0\n",
            "(1596, 2697)\n",
            "The label for the patch is: 0\n",
            "(1894, 2951)\n",
            "The label for the patch is: 0\n",
            "(1437, 3215)\n",
            "The label for the patch is: 0\n",
            "(1025, 2698)\n",
            "The label for the patch is: 0\n",
            "(1407, 2319)\n",
            "The label for the patch is: 0\n",
            "(821, 2052)\n",
            "The label for the patch is: 0\n",
            "(1447, 1606)\n",
            "The label for the patch is: 0\n",
            "(864, 2708)\n",
            "The label for the patch is: 0\n",
            "(2019, 1832)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 5 patches!\n",
            "(2089, 2760)\n",
            "The label for the patch is: 0\n",
            "(1453, 2564)\n",
            "The label for the patch is: 0\n",
            "(1867, 2730)\n",
            "The label for the patch is: 0\n",
            "(1192, 1734)\n",
            "The label for the patch is: 0\n",
            "(2357, 2076)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 6 patches!\n",
            "(1949, 951)\n",
            "The label for the patch is: 0\n",
            "(2082, 1197)\n",
            "The label for the patch is: 0\n",
            "(1301, 2785)\n",
            "The label for the patch is: 0\n",
            "(1497, 2350)\n",
            "The label for the patch is: 0\n",
            "(1921, 2432)\n",
            "The label for the patch is: 0\n",
            "(581, 2284)\n",
            "The label for the patch is: 0\n",
            "(1235, 2938)\n",
            "The label for the patch is: 0\n",
            "(1449, 818)\n",
            "The label for the patch is: 0\n",
            "(2448, 1510)\n",
            "The label for the patch is: 0\n",
            "(1155, 2309)\n",
            "The label for the patch is: 0\n",
            "(1348, 2267)\n",
            "The label for the patch is: 0\n",
            "(1007, 2899)\n",
            "The label for the patch is: 0\n",
            "(2332, 1640)\n",
            "The label for the patch is: 0\n",
            "(1381, 2454)\n",
            "The label for the patch is: 0\n",
            "(1785, 2263)\n",
            "The label for the patch is: 0\n",
            "(2278, 2874)\n",
            "The label for the patch is: 0\n",
            "(2086, 1797)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 7 patches!\n",
            "(2405, 2493)\n",
            "The label for the patch is: 0\n",
            "(1699, 2947)\n",
            "The label for the patch is: 0\n",
            "(2024, 2819)\n",
            "The label for the patch is: 0\n",
            "(2187, 2748)\n",
            "The label for the patch is: 0\n",
            "(1086, 2719)\n",
            "The label for the patch is: 0\n",
            "(2405, 1755)\n",
            "The label for the patch is: 0\n",
            "(1916, 2365)\n",
            "The label for the patch is: 0\n",
            "(1764, 1107)\n",
            "The label for the patch is: 0\n",
            "(1298, 1935)\n",
            "The label for the patch is: 0\n",
            "(1992, 2083)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 8 patches!\n",
            "(1312, 1319)\n",
            "The label for the patch is: 0\n",
            "(2746, 1490)\n",
            "The label for the patch is: 0\n",
            "(1555, 2505)\n",
            "The label for the patch is: 0\n",
            "(801, 2438)\n",
            "The label for the patch is: 0\n",
            "(1848, 2482)\n",
            "The label for the patch is: 0\n",
            "(1490, 2055)\n",
            "The label for the patch is: 0\n",
            "(2012, 2725)\n",
            "The label for the patch is: 0\n",
            "(1078, 2296)\n",
            "The label for the patch is: 0\n",
            "(2553, 1281)\n",
            "The label for the patch is: 0\n",
            "(982, 1317)\n",
            "The label for the patch is: 0\n",
            "(978, 2024)\n",
            "The label for the patch is: 0\n",
            "(1531, 1480)\n",
            "The label for the patch is: 0\n",
            "(2427, 2621)\n",
            "The label for the patch is: 0\n",
            "(2062, 2311)\n",
            "The label for the patch is: 0\n",
            "(927, 1620)\n",
            "The label for the patch is: 0\n",
            "(831, 2195)\n",
            "The label for the patch is: 0\n",
            "(1518, 2265)\n",
            "The label for the patch is: 0\n",
            "(1829, 1836)\n",
            "The label for the patch is: 0\n",
            "(1618, 2732)\n",
            "The label for the patch is: 0\n",
            "(1473, 1609)\n",
            "The label for the patch is: 0\n",
            "(2087, 2138)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 9 patches!\n",
            "(1545, 2935)\n",
            "The label for the patch is: 0\n",
            "(872, 2084)\n",
            "The label for the patch is: 0\n",
            "(995, 1811)\n",
            "The label for the patch is: 0\n",
            "(2626, 2175)\n",
            "The label for the patch is: 0\n",
            "(1973, 2337)\n",
            "The label for the patch is: 0\n",
            "(1330, 2385)\n",
            "The label for the patch is: 0\n",
            "(1794, 1939)\n",
            "The label for the patch is: 0\n",
            "(571, 2243)\n",
            "The label for the patch is: 0\n",
            "(1600, 2409)\n",
            "The label for the patch is: 0\n",
            "(1237, 1815)\n",
            "The label for the patch is: 0\n",
            "(1163, 2756)\n",
            "The label for the patch is: 0\n",
            "(1049, 1607)\n",
            "The label for the patch is: 0\n",
            "(1790, 1734)\n",
            "The label for the patch is: 0\n",
            "(938, 2086)\n",
            "The label for the patch is: 0\n",
            "(1476, 1597)\n",
            "The label for the patch is: 0\n",
            "(426, 2110)\n",
            "The label for the patch is: 0\n",
            "(1457, 1338)\n",
            "The label for the patch is: 0\n",
            "(892, 2770)\n",
            "The label for the patch is: 0\n",
            "(538, 2153)\n",
            "The label for the patch is: 0\n",
            "(1686, 2360)\n",
            "The label for the patch is: 0\n",
            "(2263, 2906)\n",
            "The label for the patch is: 0\n",
            "(434, 2259)\n",
            "The label for the patch is: 0\n",
            "(1225, 1466)\n",
            "The label for the patch is: 0\n",
            "(2442, 2393)\n",
            "The label for the patch is: 1\n",
            "train_1 add one patch and current have 10 patches!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKWUK8ohTclQ",
        "colab_type": "text"
      },
      "source": [
        "## 1.3 Save training patches and images on Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1X5uQ05Tian",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "train_dir = 'drive/My Drive/project-adl/train'\n",
        "if not os.path.exists(train_dir):\n",
        "  os.mkdir(train_dir)\n",
        "train_0_dir = os.path.join(train_dir, 'patches0')\n",
        "train_1_dir = os.path.join(train_dir, 'patches1')\n",
        "if not os.path.exists(train_0_dir):\n",
        "  os.mkdir(train_0_dir)\n",
        "if not os.path.exists(train_1_dir):\n",
        "  os.mkdir(train_1_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5XaynKEKYQO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "9b91a5e2-773a-4bb6-eabf-35c10a344f9b"
      },
      "source": [
        "print(np.shape(train_1))\n",
        "#plt.figure(figsize=(10,10),dpi=100)\n",
        "plt.imshow(train_1[5])\n",
        "from matplotlib.image import imsave\n",
        "for i in range(len(train_0)):\n",
        "  filename = os.path.join(train_0_dir,'tumor_%s_%d.png' %(slide_id, i))\n",
        "  imsave(filename,train_0[1])\n",
        "  \n",
        "for i in range(len(train_1)):\n",
        "  filename = os.path.join(train_1_dir,'tumor_%s_%d.png' %(slide_id, i))\n",
        "  imsave(filename,train_1[1])"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 299, 299, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH/hiAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzfz5ajzp803",
        "colab_type": "text"
      },
      "source": [
        "# STEP 2. MODELING\n",
        "\n",
        "* Although transfer learning may be less effective than training a model from scratch , it is the best place to start.\n",
        "* Choose a model previously trained on Imagenet. Use the techniques in Chapter 5 of Francois’s book to try transfer learning (add a single Dense layer on top of that model), and train it on your own data.\n",
        "* Write a script that takes your trained model and a testing image, and outputs a heat map showing the cancerous regions.\n",
        "* Design an evaluation metric, write a script to report your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quvembFyIxcX",
        "colab_type": "text"
      },
      "source": [
        "加快速度，不看背景，只看tissue。比如对色彩做个threshold\n",
        "\n",
        "选一个特别放大的开始做，比如800\\*600 pixel图片\n",
        "annotation. 选择哪部分有cancer\n",
        "\n",
        "image segmentation, \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd8PiefWVd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_SHAPE = 299\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpzJ1rL2p0Lh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "conv_base = InceptionV3(weights='imagenet',include_top=False, input_shape=(299,299,3))\n",
        "#conv_base.summary()\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 8, 8, 2048)) # get from conv_base.summary()\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)       \n",
        "        features[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = features_batch\n",
        "        labels[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = labels_batch\n",
        "        i += 1\n",
        "        if i * BATCH_SIZE >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, nclass*2)\n",
        "print(train_labels)\n",
        "\n",
        "FLATTENED_SHAPE = 8 * 8 * 2048\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yPwaYhpO_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1839
        },
        "outputId": "b1dcb9b2-7930-4316-be88-ed4fa541dcde"
      },
      "source": [
        "train_features = np.reshape(train_features, (nclass*2, FLATTENED_SHAPE))\n",
        "EPOCHS = 50\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=FLATTENED_SHAPE))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history3 = model.fit(train_features, train_labels,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/50\n",
            "20/20 [==============================] - 1s 68ms/sample - loss: 0.9534 - acc: 0.4500\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 2.5767 - acc: 0.7500\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 0.1539 - acc: 0.9500\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.8709e-07 - acc: 1.0000\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 0s 2ms/sample - loss: 1.0960e-07 - acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC0CYzU1qDCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}