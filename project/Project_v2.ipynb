{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project_v2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruU23naDpRx6",
        "colab_type": "text"
      },
      "source": [
        "# 4998 Applied Deep Learning Project\n",
        "## Detect Cancer Metastases on Pathology Images\n",
        "\n",
        "Jing Qian (jq2282)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiEGCtNYp05o",
        "colab_type": "text"
      },
      "source": [
        "# STEP 1. First glance of data and generate training images\n",
        "\n",
        "## Step 1 is processed in GenerateTrain.ipynb.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or8nAcNS5gIw",
        "colab_type": "code",
        "outputId": "8296d9aa-6c14-4b4b-a1e0-83414c9fb123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90ZIwbxyCNWD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_path = '/content/drive/My Drive/project-adl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0bDf_ZHzSFQz",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from openslide import open_slide, __library_version__ as openslide_version\n",
        "import os\n",
        "from PIL import Image\n",
        "from skimage.color import rgb2gray"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWy9EgLeGCOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 299 # in corresponding level\n",
        "tumor_check_size = 128 # in level 0 with the highest resolution"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzfz5ajzp803",
        "colab_type": "text"
      },
      "source": [
        "# STEP 2. MODELING\n",
        "\n",
        "* Although transfer learning may be less effective than training a model from scratch , it is the best place to start.\n",
        "* Choose a model previously trained on Imagenet. Use the techniques in Chapter 5 of Francois’s book to try transfer learning (add a single Dense layer on top of that model), and train it on your own data.\n",
        "* Write a script that takes your trained model and a testing image, and outputs a heat map showing the cancerous regions.\n",
        "* Design an evaluation metric, write a script to report your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quvembFyIxcX",
        "colab_type": "text"
      },
      "source": [
        "加快速度，不看背景，只看tissue。比如对色彩做个threshold\n",
        "\n",
        "选一个特别放大的开始做，比如800\\*600 pixel图片\n",
        "annotation. 选择哪部分有cancer\n",
        "\n",
        "image segmentation, \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdd8PiefWVd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TARGET_SHAPE = 299\n",
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tImbyWYzpeQ",
        "colab_type": "text"
      },
      "source": [
        "## 2.0 Get Training Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j1Go6i4zt21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dir = 'drive/My Drive/project-adl/train_level4'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjVn9g8O4b68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nclass=200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8656v7chqeA",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpzJ1rL2p0Lh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "8d0519b8-49c0-48a4-ad5d-75c4e821c60f"
      },
      "source": [
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "conv_base = InceptionV3(weights='imagenet',include_top=False, input_shape=(299,299,3))\n",
        "#conv_base.summary()\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "def extract_features(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 8, 8, 2048)) # get from conv_base.summary()\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base.predict(inputs_batch)       \n",
        "        features[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = features_batch\n",
        "        labels[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = labels_batch\n",
        "        i += 1\n",
        "        if i * BATCH_SIZE >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features(train_dir, nclass*2)\n",
        "print(train_labels)\n",
        "\n",
        "FLATTENED_SHAPE = 8 * 8 * 2048\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400 images belonging to 2 classes.\n",
            "[1. 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0.\n",
            " 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
            " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 1. 0. 0. 1.\n",
            " 1. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1.\n",
            " 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1.\n",
            " 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1.\n",
            " 0. 0. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1.\n",
            " 1. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 1. 0. 0.\n",
            " 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1.\n",
            " 0. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0.\n",
            " 0. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 1.\n",
            " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1.\n",
            " 0. 0. 1. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0.\n",
            " 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 1.\n",
            " 1. 1. 1. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9yPwaYhpO_0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1754
        },
        "outputId": "513129cf-90bf-46ce-d25e-0f1e6ee2d8ae"
      },
      "source": [
        "train_features = np.reshape(train_features, (nclass*2, FLATTENED_SHAPE))\n",
        "EPOCHS = 50\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=FLATTENED_SHAPE))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history3 = model.fit(train_features, train_labels,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_split = 0.2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/50\n",
            "320/320 [==============================] - 1s 4ms/sample - loss: 2.9683 - acc: 0.7406 - val_loss: 1.7289 - val_acc: 0.8875\n",
            "Epoch 2/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.4292 - acc: 0.8375 - val_loss: 1.3472 - val_acc: 0.9125\n",
            "Epoch 3/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.6834 - acc: 0.8281 - val_loss: 1.1755 - val_acc: 0.9250\n",
            "Epoch 4/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.3100 - acc: 0.8531 - val_loss: 1.5942 - val_acc: 0.9000\n",
            "Epoch 5/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.9793 - acc: 0.8719 - val_loss: 1.0008 - val_acc: 0.9375\n",
            "Epoch 6/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.6346 - acc: 0.8344 - val_loss: 1.1300 - val_acc: 0.9250\n",
            "Epoch 7/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.4059 - acc: 0.8469 - val_loss: 1.7164 - val_acc: 0.8875\n",
            "Epoch 8/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 3.3413 - acc: 0.7906 - val_loss: 6.1409 - val_acc: 0.6125\n",
            "Epoch 9/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 3.1151 - acc: 0.7969 - val_loss: 1.5986 - val_acc: 0.9000\n",
            "Epoch 10/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.0337 - acc: 0.8687 - val_loss: 1.4015 - val_acc: 0.9125\n",
            "Epoch 11/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 3.9285 - acc: 0.7500 - val_loss: 4.7816 - val_acc: 0.7000\n",
            "Epoch 12/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 6.1846 - acc: 0.6156 - val_loss: 4.4252 - val_acc: 0.7250\n",
            "Epoch 13/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 3.5113 - acc: 0.7750 - val_loss: 1.2001 - val_acc: 0.9250\n",
            "Epoch 14/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.5799 - acc: 0.8375 - val_loss: 3.3940 - val_acc: 0.7875\n",
            "Epoch 15/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 3.3911 - acc: 0.7875 - val_loss: 3.5871 - val_acc: 0.7750\n",
            "Epoch 16/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.7066 - acc: 0.8250 - val_loss: 1.7957 - val_acc: 0.8875\n",
            "Epoch 17/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.7929 - acc: 0.8813 - val_loss: 1.6110 - val_acc: 0.8875\n",
            "Epoch 18/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.8223 - acc: 0.8813 - val_loss: 1.2001 - val_acc: 0.9250\n",
            "Epoch 19/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.7440 - acc: 0.8906 - val_loss: 1.7053 - val_acc: 0.8875\n",
            "Epoch 20/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 2.1221 - acc: 0.8656 - val_loss: 1.1979 - val_acc: 0.9250\n",
            "Epoch 21/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.6999 - acc: 0.8938 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 22/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.9518 - acc: 0.8781 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 23/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.7650 - acc: 0.8875 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 24/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.5220 - acc: 0.9000 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 25/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.7722 - acc: 0.8875 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 26/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.4481 - acc: 0.9094 - val_loss: 1.1979 - val_acc: 0.9250\n",
            "Epoch 27/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.4085 - acc: 0.9094 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 28/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.5505 - acc: 0.9031 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 29/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.4337 - acc: 0.9094 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 30/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.3461 - acc: 0.9125 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 31/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.2110 - acc: 0.9219 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 32/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1544 - acc: 0.9250 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 33/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1152 - acc: 0.9281 - val_loss: 0.9986 - val_acc: 0.9375\n",
            "Epoch 34/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1727 - acc: 0.9219 - val_loss: 1.0008 - val_acc: 0.9375\n",
            "Epoch 35/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.3815 - acc: 0.9031 - val_loss: 0.9964 - val_acc: 0.9375\n",
            "Epoch 36/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.7362 - acc: 0.8906 - val_loss: 0.9964 - val_acc: 0.9375\n",
            "Epoch 37/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 0.9980 - acc: 0.9375 - val_loss: 0.9964 - val_acc: 0.9375\n",
            "Epoch 38/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1921 - acc: 0.9250 - val_loss: 0.7993 - val_acc: 0.9500\n",
            "Epoch 39/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.0019 - acc: 0.9375 - val_loss: 0.7993 - val_acc: 0.9500\n",
            "Epoch 40/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.0013 - acc: 0.9375 - val_loss: 0.7994 - val_acc: 0.9500\n",
            "Epoch 41/50\n",
            "320/320 [==============================] - 0s 2ms/sample - loss: 1.1010 - acc: 0.9312 - val_loss: 0.8518 - val_acc: 0.9375\n",
            "Epoch 42/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1519 - acc: 0.9281 - val_loss: 0.8933 - val_acc: 0.9375\n",
            "Epoch 43/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.0787 - acc: 0.9312 - val_loss: 0.9104 - val_acc: 0.9375\n",
            "Epoch 44/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.0586 - acc: 0.9312 - val_loss: 0.7971 - val_acc: 0.9500\n",
            "Epoch 45/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 0.9997 - acc: 0.9375 - val_loss: 0.7971 - val_acc: 0.9500\n",
            "Epoch 46/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 0.9605 - acc: 0.9375 - val_loss: 0.7993 - val_acc: 0.9500\n",
            "Epoch 47/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.0506 - acc: 0.9344 - val_loss: 0.7993 - val_acc: 0.9500\n",
            "Epoch 48/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 0.9516 - acc: 0.9406 - val_loss: 0.7993 - val_acc: 0.9500\n",
            "Epoch 49/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.2332 - acc: 0.9219 - val_loss: 0.6711 - val_acc: 0.9500\n",
            "Epoch 50/50\n",
            "320/320 [==============================] - 1s 2ms/sample - loss: 1.1522 - acc: 0.9281 - val_loss: 0.7993 - val_acc: 0.9500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eC0CYzU1qDCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VRzvRxxhuDQ",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Train from scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJdyzmT0hwyp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv_base_2 = InceptionV3(weights=None,include_top=False, input_shape=(299,299,3))\n",
        "#conv_base_2.summary()\n",
        "datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "def extract_features_2(directory, sample_count):\n",
        "    features = np.zeros(shape=(sample_count, 8, 8, 2048)) # get from conv_base.summary()\n",
        "    labels = np.zeros(shape=(sample_count))\n",
        "    generator = datagen.flow_from_directory(\n",
        "        directory,\n",
        "        target_size=(TARGET_SHAPE, TARGET_SHAPE),\n",
        "        batch_size=BATCH_SIZE,\n",
        "        class_mode='binary')\n",
        "    i = 0\n",
        "    for inputs_batch, labels_batch in generator:\n",
        "        features_batch = conv_base_2.predict(inputs_batch)       \n",
        "        features[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = features_batch\n",
        "        labels[i * BATCH_SIZE : (i + 1) * BATCH_SIZE] = labels_batch\n",
        "        i += 1\n",
        "        if i * BATCH_SIZE >= sample_count:\n",
        "            break\n",
        "    return features, labels\n",
        "\n",
        "train_features, train_labels = extract_features_2(train_dir, nclass*2)\n",
        "print(train_labels)\n",
        "\n",
        "FLATTENED_SHAPE = 8 * 8 * 2048"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z79AD-Rimcfc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = np.reshape(train_features, (nclass*2, FLATTENED_SHAPE))\n",
        "EPOCHS = 50\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_dim=FLATTENED_SHAPE))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history3 = model.fit(train_features, train_labels,\n",
        "                    epochs=EPOCHS,\n",
        "                    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQnz4-hEbyTz",
        "colab_type": "text"
      },
      "source": [
        "## 1.4 Generate test patches and corresponding labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGDgFYlcb8C8",
        "colab_type": "text"
      },
      "source": [
        "### Start with one slide"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "90d14b9a-6d1d-4974-efe0-acc4028fa45e",
        "id": "tGuT0pXnSFpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# Install the OpenSlide C library and Python bindings\n",
        "!apt-get install openslide-tools\n",
        "!pip install openslide-python"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openslide-tools is already the newest version (3.4.1+dfsg-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "Requirement already satisfied: openslide-python in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from openslide-python) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->openslide-python) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o6-f4hX6HKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As mentioned in class, we can improve efficiency by ignoring non-tissue areas \n",
        "# of the slide. We'll find these by looking for all gray regions.\n",
        "def find_tissue_pixels(image, intensity=0.8):\n",
        "    im_gray = rgb2gray(image)\n",
        "    assert im_gray.shape == (image.shape[0], image.shape[1])\n",
        "    indices = np.where(im_gray <= intensity)\n",
        "    return zip(indices[0], indices[1])\n",
        "\n",
        "# This function has nothing to do with tumor mask, but shows tissue region\n",
        "def apply_mask(im, mask, color=(255,0,0)):\n",
        "    masked = np.copy(im)\n",
        "    for x,y in mask: masked[x][y] = color\n",
        "    return masked"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ASxFrOl_iXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# See https://openslide.org/api/python/#openslide.OpenSlide.read_region\n",
        "# Note: x,y coords are with respect to level 0.\n",
        "# There is an example below of working with coordinates\n",
        "# with respect to a higher zoom level.\n",
        "\n",
        "# Read a region from the slide\n",
        "# Return a numpy RBG array\n",
        "def read_slide(slide, x, y, level, width, height, as_float=False):\n",
        "    #print('1')\n",
        "    im = slide.read_region((x,y), level, (width, height))\n",
        "    #print('2')\n",
        "    im = im.convert('RGB') # drop the alpha channel\n",
        "    #print('3')\n",
        "    if as_float:\n",
        "        im = np.asarray(im, dtype=np.float32)\n",
        "    else:\n",
        "        im = np.asarray(im)\n",
        "    assert im.shape == (height, width, 3)\n",
        "    return im"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}